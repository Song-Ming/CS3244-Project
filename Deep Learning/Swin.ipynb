{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9aca3d-cb6f-4801-9f88-a1c3f43a0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchinfo as info\n",
    "import torchvision.transforms.v2 as v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7069ad6-8762-4612-9cd7-82e1a0a3915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed(600)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d754c8-3fde-4553-a8b9-d001b8e546af",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()\n",
    "labels_dir = os.path.join(wd,'Labels')\n",
    "img_dir = os.path.join(wd,'Sets')\n",
    "record_path = os.path.join(wd,'Records')\n",
    "model_path = os.path.join(wd,'Models')\n",
    "if not os.path.exists(record_path):\n",
    "    os.makedirs(record_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17a8b66-dbd7-4fd5-bd6e-1174c4b0d2b2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5881e8-ed4f-4259-95f7-fd5cc952824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import swin_s, Swin_S_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba08aef-f362-4541-83f2-7a65a8d6623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = swin_s(weights = Swin_S_Weights.IMAGENET1K_V1).to(device)\n",
    "transform = Swin_S_Weights.IMAGENET1K_V1.transforms()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5adfb4b0-e729-4634-9b8c-6444abb33d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "layers.append(nn.Linear(in_features = 768, out_features = 256, bias = True))\n",
    "layers.append(nn.Dropout(p = 0.15))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(in_features = 256, out_features = 256, bias = True))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(in_features = 256, out_features = 9, bias = True))\n",
    "model.head = nn.Sequential(*layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c79148-45a2-484e-83c8-4167816000ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.features[7][0].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.features[7][1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddc0d0eb-2fc3-438d-8393-552d8cdc4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#info.summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de01fe8-f450-4559-9bd3-d760900f1bfa",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15bcb2ee-cb99-46d3-8d46-2987a5579c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeedsDataset():\n",
    "    def __init__(self, labels_dir, img_dir, transform):\n",
    "        self.train_labels = pd.read_csv(os.path.join(labels_dir,'train.csv'))\n",
    "        self.valid_labels = pd.read_csv(os.path.join(labels_dir,'valid.csv'))\n",
    "        self.test_labels = pd.read_csv(os.path.join(labels_dir,'test.csv'))\n",
    "        self.img_dir = img_dir\n",
    "        self.classes = None\n",
    "        self.counts = {'train':{},'valid':{},'test':{}}\n",
    "        self.train = []\n",
    "        self.valid = []\n",
    "        self.test = []\n",
    "        self.augmented = []\n",
    "        self.transform = transform\n",
    "        \n",
    "    # Load dataset\n",
    "    def __loaddata__(self):\n",
    "        self.classes = self.train_labels[['Label','Class']].drop_duplicates().sort_values(by = 'Label').reset_index(drop = True)['Class']\n",
    "        for key in self.classes.keys():\n",
    "            self.counts['train'][key] = 0\n",
    "            self.counts['valid'][key] = 0\n",
    "            self.counts['test'][key] = 0\n",
    "\n",
    "        for row in self.train_labels.itertuples():\n",
    "            filename = row.Filename\n",
    "            label = row.Label\n",
    "            self.counts['train'][row.Label] += 1\n",
    "            img_path = os.path.join(self.img_dir,'train',filename)\n",
    "            image = torchvision.io.read_image(img_path)\n",
    "            self.train.append([image,label])   \n",
    "        for row in self.valid_labels.itertuples():\n",
    "            filename = row.Filename\n",
    "            label = row.Label\n",
    "            self.counts['valid'][row.Label] += 1\n",
    "            img_path = os.path.join(self.img_dir,'valid',filename)\n",
    "            image = torchvision.io.read_image(img_path)\n",
    "            self.valid.append([image,label])   \n",
    "        for row in self.test_labels.itertuples():\n",
    "            filename = row.Filename\n",
    "            label = row.Label\n",
    "            self.counts['test'][row.Label] += 1\n",
    "            img_path = os.path.join(self.img_dir,'test',filename)\n",
    "            image = torchvision.io.read_image(img_path)\n",
    "            self.test.append([image,label])\n",
    "\n",
    "        del self.train_labels,self.valid_labels,self.test_labels\n",
    "        print('Data has been loaded')\n",
    "\n",
    "    def __apply__(self):\n",
    "        for i in range(len(self.train)):\n",
    "            image = transform(self.train[i][0])\n",
    "            label = torch.tensor(self.train[i][1])\n",
    "            self.train[i] = [image,label]\n",
    "\n",
    "        for i in range(len(self.valid)):\n",
    "            image = transform(self.valid[i][0])\n",
    "            label = torch.tensor(self.valid[i][1])\n",
    "            self.valid[i] = [image,label]\n",
    "\n",
    "        for i in range(len(self.test)):\n",
    "            image = transform(self.test[i][0])\n",
    "            label = torch.tensor(self.test[i][1])\n",
    "            self.test[i] = [image,label]\n",
    "        print('Data has been processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3f57817-58f8-4272-a61a-a1fec70f49c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded\n",
      "Data has been processed\n"
     ]
    }
   ],
   "source": [
    "weeds = WeedsDataset(labels_dir,img_dir,transform)\n",
    "weeds.__loaddata__()\n",
    "weeds.__apply__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ed7446-f62b-46ee-8445-45fcccf69ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(weeds.counts)\n",
    "#print(weeds.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a69f8f-5cff-46ae-9a40-de0b5ea68ae4",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de1d5ec-6cf4-42cc-bb79-da40c3812083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, current_index, batch):\n",
    "    images,labels = zip(*dataset[current_index:current_index+batch])\n",
    "    return torch.stack(images, dim = 0).to(device), torch.stack(labels).to(device)\n",
    "    \n",
    "def train_one_epoch(model, dataset, batch_size):\n",
    "    random.shuffle(dataset.train)\n",
    "    batches = math.floor(len(dataset.train)/batch_size)\n",
    "    current_index = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(dataset.train, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.cpu().item()\n",
    "\n",
    "    remainder = len(dataset.train)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(dataset.train, current_index, remainder)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.cpu().item()\n",
    "        batches += 1\n",
    "    \n",
    "    return running_loss/batches\n",
    "\n",
    "def validate(model, dataset, batch_size):\n",
    "    batches = math.floor(len(dataset.valid)/batch_size)\n",
    "    current_index = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(dataset.valid, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        output = model(inputs)\n",
    "        running_loss += loss_fn(output, labels).cpu().item()\n",
    "\n",
    "    remainder = len(dataset.valid)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(dataset.valid, current_index, remainder)\n",
    "        output = model(inputs)\n",
    "        running_loss += loss_fn(output, labels).cpu().item()\n",
    "        batches += 1\n",
    "    \n",
    "    return running_loss/batches\n",
    "\n",
    "def train(epochs, model, dataset, batch_size, folder_name, epoch_number, save, record):\n",
    "    best_vloss = 1000\n",
    "    current_vloss = 0\n",
    "    epoch_counter = epoch_number + 1\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    if not os.path.exists('{}\\{}'.format(model_path,folder_name)):\n",
    "        os.makedirs('{}\\{}'.format(model_path,folder_name))\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}:'.format(epoch_counter))\n",
    "        model.train()\n",
    "        avg_loss = train_one_epoch(model, dataset, batch_size)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            current_vloss = validate(model, dataset, batch_size)\n",
    "\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, current_vloss))\n",
    "        if current_vloss <= best_vloss:\n",
    "                best_vloss = current_vloss\n",
    "                print('New best validation loss')\n",
    "\n",
    "        if epoch_counter - epoch_number > epochs - save:\n",
    "            path = '{}\\\\{}\\\\model_{}'.format(model_path, folder_name, epoch_counter)\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "        record.append([epoch_counter,avg_loss,current_vloss])\n",
    "        epoch_counter += 1\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print('Average time taken per epoch is {}s'.format((end - start)/epochs))\n",
    "    return\n",
    "\n",
    "def compute_weights(dataset):\n",
    "    n_samples = len(dataset.train)\n",
    "    n_classes = len(dataset.classes)\n",
    "    weights = torch.zeros(n_classes)\n",
    "    dict = dataset.counts['train']\n",
    "    for i in range(n_classes):\n",
    "        weight = n_samples/(n_classes*dict[i])\n",
    "        weights[i] = weight\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f4c59db-fa15-49b9-9fdb-8d99ae183014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula for F1: (2 * FP) / (2 * TP + FP + FN) OR 2 * (Precision * Recall) / (Precision + Recall)\n",
    "# Formula for Precision: TP / (TP + FP)\n",
    "# Formula for Recall: TP / (TP + FN)\n",
    "def metrics(TP,FP,FN):\n",
    "    if TP > 0:\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1  = TP / (TP + 0.5 * (FP + FN))\n",
    "    else:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        F1  = 0\n",
    "    return precision, recall, F1\n",
    "    \n",
    "# Use weeds.dataset for dataset\n",
    "def evaluate(model, dataset, batch_size, classes):\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    batches = math.floor(len(dataset) / batch_size)\n",
    "    current_index = 0\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i in range(batches):\n",
    "        inputs, labels = get_batch(dataset, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        prediction = torch.argmax(model(inputs),dim = 1)\n",
    "        \n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += batch_size\n",
    "\n",
    "        all_preds.extend(prediction.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    remainder = len(dataset) % batch_size\n",
    "    if remainder != 0:\n",
    "        inputs, labels = get_batch(dataset, current_index, remainder)\n",
    "        prediction = torch.argmax(model(inputs),dim = 1)\n",
    "        \n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += remainder\n",
    "\n",
    "        all_preds.extend(prediction.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Compute Metrics for individual classes\n",
    "    num_classes = len(classes) \n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds, labels=np.arange(num_classes))\n",
    "    print(conf_matrix)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        TP = conf_matrix[i, i]  # True Positives\n",
    "        FP = np.sum(conf_matrix[:, i]) - TP  # False Positives\n",
    "        FN = np.sum(conf_matrix[i, :]) - TP  # False Negatives\n",
    "        precision,recall,F1 = metrics(TP, FP, FN)\n",
    "        print(f\"Precision for class {classes[i]}: {precision:.4f}\")\n",
    "        print(f\"Recall for class {classes[i]}: {recall:.4f}\")\n",
    "        print(f\"F1-score for class {classes[i]}: {F1:.4f}\")\n",
    "    print('Total Accuracy is {}%'.format(accuracy))\n",
    "    \n",
    "    end = time.perf_counter()\n",
    "    print('Time taken is {}'.format(end - start))\n",
    "\n",
    "def inference_time(model, dataset, batch_size, num_samples):\n",
    "    model.eval()\n",
    "    sample = random.sample(dataset, num_samples)\n",
    "    batches = int(num_samples/batch_size)\n",
    "    current_index = 0\n",
    "\n",
    "    print(f\"\\nüîç Measuring inference time for {num_samples} images...\\n\")\n",
    "    start = time.perf_counter()\n",
    "    for i in range(batches):\n",
    "        inputs, labels = get_batch(sample,current_index,batch_size)\n",
    "        current_index += batch_size\n",
    "        with torch.no_grad():\n",
    "            prediction = torch.argmax(model(inputs),dim = 1)\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    total_time = end - start\n",
    "    print(f\"\\nInference Time is {total_time:.5f} seconds for {num_samples} images...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178bc7f8-0bb6-4382-8c1a-d56ee4ee55a6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d3b6956-9e11-43a4-83a5-123ba29eb3db",
   "metadata": {},
   "source": [
    "weights = compute_weights(weeds)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e96214b-1221-4491-8f15-787ef7f907cf",
   "metadata": {},
   "source": [
    "record = [['Epoch','Training loss','Validation loss']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f995363-d3a6-44b6-bddf-d9e13264a57e",
   "metadata": {},
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight=weights).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.6, weight_decay = 0.00001)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f691af3d-2c33-4391-9453-c6bbed49e9ce",
   "metadata": {},
   "source": [
    "train(30,model,weeds,30,'swin1',0,30,record)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84844565-2412-43d2-8615-78b1ef7f6907",
   "metadata": {},
   "source": [
    "path = os.path.join(model_path,'swin1','model_22')\n",
    "model.load_state_dict(torch.load(path, weights_only = True))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e70959d-d602-44cb-a639-97d3ad0ac5ab",
   "metadata": {},
   "source": [
    "record = record[:23]\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.004, momentum = 0.6, weight_decay = 0.00001)\n",
    "train(18,model,weeds,30,'swin1',22,18,record)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a0d6058-bcd6-42eb-9db9-31b9bbf34221",
   "metadata": {},
   "source": [
    "with open(os.path.join(record_path,'swin1.csv'), 'w', newline='') as csvAP:\n",
    "    writer = csv.writer(csvAP)\n",
    "    writer.writerows(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c3a63-5626-4bc8-b719-4fe8d7fe6ff6",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30acc7cb-f14b-4672-b251-5dd509b4c870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(model_path,'swin1','model_24')\n",
    "model.load_state_dict(torch.load(path, weights_only = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63e4f4bb-1a7d-47f1-b2a2-3b58a8d0f313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 89   4   0   0   0   2   0   3   2]\n",
      " [  0 100   0   0   0   0   0   2   1]\n",
      " [  0   0 118   0   1   0   0   0   1]\n",
      " [  1   0   1  89   0   0   0   0   2]\n",
      " [  0   0   1   0 101   0   0   0   0]\n",
      " [  1   0   0   0   0 107   0   0   0]\n",
      " [  0   0   0   0   0   0 108   0   2]\n",
      " [  2   0   0   0   0   0   0 113   4]\n",
      " [  3   6   2   6   5   8   3   3 860]]\n",
      "Precision for class Chinee apple: 0.9271\n",
      "Recall for class Chinee apple: 0.8900\n",
      "F1-score for class Chinee apple: 0.9082\n",
      "Precision for class Lantana: 0.9091\n",
      "Recall for class Lantana: 0.9709\n",
      "F1-score for class Lantana: 0.9390\n",
      "Precision for class Parkinsonia: 0.9672\n",
      "Recall for class Parkinsonia: 0.9833\n",
      "F1-score for class Parkinsonia: 0.9752\n",
      "Precision for class Parthenium: 0.9368\n",
      "Recall for class Parthenium: 0.9570\n",
      "F1-score for class Parthenium: 0.9468\n",
      "Precision for class Prickly acacia: 0.9439\n",
      "Recall for class Prickly acacia: 0.9902\n",
      "F1-score for class Prickly acacia: 0.9665\n",
      "Precision for class Rubber vine: 0.9145\n",
      "Recall for class Rubber vine: 0.9907\n",
      "F1-score for class Rubber vine: 0.9511\n",
      "Precision for class Siam weed: 0.9730\n",
      "Recall for class Siam weed: 0.9818\n",
      "F1-score for class Siam weed: 0.9774\n",
      "Precision for class Snake weed: 0.9339\n",
      "Recall for class Snake weed: 0.9496\n",
      "F1-score for class Snake weed: 0.9417\n",
      "Precision for class Negative: 0.9862\n",
      "Recall for class Negative: 0.9598\n",
      "F1-score for class Negative: 0.9729\n",
      "Total Accuracy is 96.23072529982866%\n",
      "Time taken is 6.499691899998652\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    evaluate(model,weeds.test,30,weeds.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6354469-dcff-4858-8f75-77a2fbdd44b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Measuring inference time for 240 images...\n",
      "\n",
      "\n",
      "Inference Time is 0.92128 seconds for 240 images...\n",
      "\n",
      "\n",
      "üîç Measuring inference time for 240 images...\n",
      "\n",
      "\n",
      "Inference Time is 0.79820 seconds for 240 images...\n",
      "\n",
      "\n",
      "üîç Measuring inference time for 240 images...\n",
      "\n",
      "\n",
      "Inference Time is 0.79672 seconds for 240 images...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inference_time(model,weeds.test,30,240)\n",
    "inference_time(model,weeds.test,30,240)\n",
    "inference_time(model,weeds.test,30,240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb5db6-15b8-4239-b830-50f86314b7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
