{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacfff18-b56d-47cc-8021-86b8f8941c67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchinfo as info\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as functional\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009fad0a-1e6e-492d-894b-d7c60e027a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a30476-d84d-4454-b282-1c16ad69849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790544f-fb7d-45d9-86b4-8cb420f61398",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b99fd7-d9af-4281-af60-0f49ed07b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import convnext_base, ConvNeXt_Base_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a9b2801-09fd-44b7-90ae-aa2b11d48167",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convnext_base(weights = ConvNeXt_Base_Weights.IMAGENET1K_V1).to(device)\n",
    "process = ConvNeXt_Base_Weights.IMAGENET1K_V1.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "958d5aad-f846-4847-86ad-9a1519cab348",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "layers = []\n",
    "layers.append(nn.Linear(in_features = 1024, out_features = 256, bias = True))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(in_features = 256, out_features = 9, bias = True))\n",
    "model.classifier[2] = nn.Sequential(*layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0dc49c-a3a2-45e5-be4a-492c7f10d45b",
   "metadata": {},
   "source": [
    "## Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67bfc83f-fb88-43c1-911c-74905af090bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeedsDataset():\n",
    "    def __init__(self, labels, img_dir, transform = None):\n",
    "        self.labels = pd.read_csv(labels)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.classes = self.labels[['Label','Species']].drop_duplicates().sort_values(by = 'Label').reset_index(drop = True)['Species']\n",
    "        self.data = []\n",
    "        self.train = []\n",
    "        self.validation = []\n",
    "        self.test = []\n",
    "        \n",
    "    # Load entire dataset into memory\n",
    "    def __loaddata__(self):\n",
    "        for row in self.labels.itertuples():\n",
    "            filename = row.Filename\n",
    "            label = torch.tensor(row.Label)\n",
    "            img_path = os.path.join(self.img_dir, filename)\n",
    "            image = torchvision.io.read_image(img_path)\n",
    "            self.data.append([image,label])\n",
    "        del self.labels\n",
    "        print('Data has been loaded')\n",
    "\n",
    "    # Split into train/validation/test\n",
    "    def __split__(self):\n",
    "        random.shuffle(self.data)\n",
    "        n = len(self.data)\n",
    "        n_train = round(n * 0.8)\n",
    "        n_valid = round(n * 0.1)\n",
    "        \n",
    "        self.train = self.data[:n_train]\n",
    "        self.valid = self.data[n_train:n_train + n_valid]\n",
    "        self.test = self.data[n_train + n_valid:]\n",
    "        del self.data\n",
    "        gc.collect()\n",
    "        \n",
    "        print('Data has been split')\n",
    "        print('Size of training set is {}'.format(len(self.train)))\n",
    "        print('Size of validation set is {}'.format(len(self.valid)))\n",
    "        print('Size of test set is {}'.format(len(self.test)))\n",
    "\n",
    "    # Augment training data\n",
    "    def __augment__(self, frac):\n",
    "        aug = []\n",
    "        color_jitter = transforms.ColorJitter(brightness = 0.3, contrast = 0.2, saturation = 0.3, hue = 0.0)\n",
    "        n = round(len(self.train) * frac / 3)\n",
    "        \n",
    "        rotate_90 = random.sample(self.train, n)\n",
    "        for row in rotate_90:\n",
    "            image = row[0].clone()\n",
    "            image = transforms.functional.rotate(image, 90)\n",
    "            image = color_jitter(image)\n",
    "            row[0] = image\n",
    "        \n",
    "        rotate_180 = random.sample(self.train, n)\n",
    "        for row in rotate_180:\n",
    "            image = row[0].clone()\n",
    "            image = transforms.functional.rotate(image, 180)\n",
    "            image = color_jitter(image)\n",
    "            row[0] = image\n",
    "        \n",
    "        rotate_270 = random.sample(self.train, n)\n",
    "        for row in rotate_270:\n",
    "            image = row[0].clone()\n",
    "            image = transforms.functional.rotate(image, 270)\n",
    "            image = color_jitter(image)\n",
    "            row[0] = image\n",
    "     \n",
    "        self.train += rotate_90 + rotate_180 + rotate_270  \n",
    "        print('Data has been augmented')\n",
    "        print('Size of training set is {}'.format(len(self.train)))\n",
    "\n",
    "    # Apply model transformation\n",
    "    def __apply__(self):\n",
    "        for i in range(len(self.train)):\n",
    "            self.train[i][0] = self.transform(self.train[i][0])\n",
    "        for i in range(len(self.valid)):\n",
    "            self.valid[i][0] = self.transform(self.valid[i][0])\n",
    "        for i in range(len(self.test)):\n",
    "            self.test[i][0] = self.transform(self.test[i][0]) \n",
    "        print('Data has been transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd999b99-02f7-4403-be23-349c41b8749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded\n",
      "Data has been split\n",
      "Size of training set is 14007\n",
      "Size of validation set is 1751\n",
      "Size of test set is 1751\n",
      "Data has been augmented\n",
      "Size of training set is 22410\n",
      "Data has been transformed\n"
     ]
    }
   ],
   "source": [
    "# Run functions to load and process data\n",
    "weeds = WeedsDataset(rf'{wd}\\labels.csv', rf'{wd}\\images', process)\n",
    "weeds.__loaddata__()\n",
    "weeds.__split__()\n",
    "weeds.__augment__(0.6)\n",
    "weeds.__apply__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8107d-61d6-4adf-9707-3a1b15906aa8",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e299c12-151b-457f-92cc-db73288bbc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, current_index, batch_size):\n",
    "    images,labels = zip(*dataset[current_index:current_index+batch_size])\n",
    "    return torch.stack(images, dim = 0).to(device), torch.stack(labels).to(device)\n",
    "    \n",
    "def train_one_epoch(model, dataset, batch_size):\n",
    "    batches = math.floor(len(dataset.train)/batch_size)\n",
    "    current_index = 0\n",
    "    running_loss = 0\n",
    "    random.shuffle(dataset.train)\n",
    "\n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(dataset.train, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.cpu().item()\n",
    "\n",
    "    remainder = len(dataset.train)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(dataset.train, current_index, batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.cpu().item()\n",
    "        batches += 1\n",
    "    \n",
    "    return running_loss/batches\n",
    "\n",
    "def validate(model, dataset, batch_size):\n",
    "    batches = math.floor(len(dataset.valid)/batch_size)\n",
    "    current_index = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(dataset.valid, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        output = model(inputs)\n",
    "        running_loss += loss_fn(output, labels).cpu().item()\n",
    "\n",
    "    remainder = len(dataset.valid)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(dataset.valid, current_index, batch_size)\n",
    "        output = model(inputs)\n",
    "        running_loss += loss_fn(output, labels).cpu().item()\n",
    "        batches += 1\n",
    "    \n",
    "    return running_loss/batches\n",
    "\n",
    "def train(epochs, model, dataset, batch_size, folder_name, epoch_number, save):\n",
    "    best_vloss = 1000\n",
    "    current_vloss = 0\n",
    "    epoch_counter = epoch_number + 1\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    if not os.path.exists('{}\\{}'.format(wd,folder_name)):\n",
    "        os.makedirs('{}\\{}'.format(wd,folder_name))\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}:'.format(epoch_counter))\n",
    "        model.train()\n",
    "        avg_loss = train_one_epoch(model, dataset, batch_size)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            current_vloss = validate(model, dataset, batch_size)\n",
    "\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, current_vloss))\n",
    "        if current_vloss <= best_vloss:\n",
    "                best_vloss = current_vloss\n",
    "                print('New best validation loss')\n",
    "\n",
    "        print('Lr is currently {}'.format(lr_scheduler.get_last_lr()))\n",
    "        lr_scheduler.step(current_vloss)\n",
    "\n",
    "        if epoch_counter > epochs - save:\n",
    "            model_path = '{}\\model_{}'.format(folder_name, epoch_counter)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "        epoch_counter += 1\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print('Average time taken per epoch is {}s'.format((end - start)/epochs))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68045c7b-12be-4fa7-ac5a-e5e1e317affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula for F1: (2 * FP) / (2 * TP + FP + FN) OR 2 * (Precision * Recall) / (Precision + Recall)\n",
    "# Formula for Precision: TP / (TP + FP)\n",
    "# Formula for Recall: TP / (TP + FN)\n",
    "def metrics(TP,FP,FN):\n",
    "    if TP > 0:\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1  = TP / (TP + 0.5 * (FP + FN))\n",
    "    else:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        F1  = 0\n",
    "    return precision, recall, F1\n",
    "    \n",
    "def evaluate(model, dataset, batch_size):\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    batches = math.floor(len(dataset.test) / batch_size)\n",
    "    current_index = 0\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(batches):\n",
    "        inputs, labels = get_batch(dataset.test, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        prediction = torch.argmax(model(inputs), dim=1)\n",
    "        \n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += batch_size\n",
    "\n",
    "        all_preds.extend(prediction.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    remainder = len(dataset.test) % batch_size\n",
    "    if remainder != 0:\n",
    "        inputs, labels = get_batch(dataset.test, current_index, remainder)\n",
    "        prediction = torch.argmax(model(inputs), dim=1)\n",
    "        \n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += remainder\n",
    "\n",
    "        all_preds.extend(prediction.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Compute Metrics for individual classes\n",
    "    num_classes = len(dataset.classes) \n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds, labels=np.arange(num_classes))\n",
    "    print(conf_matrix)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        TP = conf_matrix[i, i]  # True Positives\n",
    "        FP = np.sum(conf_matrix[:, i]) - TP  # False Positives\n",
    "        FN = np.sum(conf_matrix[i, :]) - TP  # False Negatives\n",
    "        precision,recall,F1 = metrics(TP, FP, FN)\n",
    "        print(f\"Precision for class {dataset.classes[i]}: {precision:.4f}\")\n",
    "        print(f\"Recall for class {dataset.classes[i]}: {recall:.4f}\")\n",
    "        print(f\"F1-score for class {dataset.classes[i]}: {F1:.4f}\")\n",
    "    print('Total Accuracy is {}%'.format(accuracy))\n",
    "    \n",
    "    end = time.perf_counter()\n",
    "    print('Time taken is {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8577d8-1d26-4b6a-9c8b-7e16335069f9",
   "metadata": {},
   "source": [
    "## Training with only classifier unfrozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f73b171-8ffe-4d09-ae58-2462d99cf720",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.6)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.5, patience = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a8c84aa-7c06-4179-980e-0bb63ee85038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 1.3151770301810353 valid 0.9443850848409865\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 2:\n",
      "LOSS train 0.9444803911487349 valid 0.7382359165284369\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 3:\n",
      "LOSS train 0.8059539428532522 valid 0.6703195091750886\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 4:\n",
      "LOSS train 0.7222074720933868 valid 0.5721244696113799\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 5:\n",
      "LOSS train 0.672411685133299 valid 0.5119041088554594\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 6:\n",
      "LOSS train 0.6272963405584705 valid 0.464963318573104\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 7:\n",
      "LOSS train 0.6002127816921353 valid 0.4481567301683956\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 8:\n",
      "LOSS train 0.5780853566056104 valid 0.41632111246387166\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 9:\n",
      "LOSS train 0.5549104350114984 valid 0.40299441417058307\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 10:\n",
      "LOSS train 0.5343360470769666 valid 0.40378315684696037\n",
      "Lr is currently [0.01]\n",
      "EPOCH 11:\n",
      "LOSS train 0.5201599192712248 valid 0.3808557416001956\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 12:\n",
      "LOSS train 0.5036448884116514 valid 0.36500372116764385\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 13:\n",
      "LOSS train 0.49144488327487273 valid 0.36399453319609165\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 14:\n",
      "LOSS train 0.47966208967705876 valid 0.3479291834971971\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 15:\n",
      "LOSS train 0.462215557462123 valid 0.35119541568888557\n",
      "Lr is currently [0.01]\n",
      "EPOCH 16:\n",
      "LOSS train 0.45595435917642974 valid 0.3536180126377278\n",
      "Lr is currently [0.01]\n",
      "EPOCH 17:\n",
      "LOSS train 0.44231014149491665 valid 0.35865017828635043\n",
      "Lr is currently [0.01]\n",
      "EPOCH 18:\n",
      "LOSS train 0.43239243325252574 valid 0.32425024908863836\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 19:\n",
      "LOSS train 0.42279330602865706 valid 0.32167549389931893\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 20:\n",
      "LOSS train 0.41390127086294254 valid 0.34461916020760935\n",
      "Lr is currently [0.01]\n",
      "EPOCH 21:\n",
      "LOSS train 0.40482785176062636 valid 0.3079831103483836\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 22:\n",
      "LOSS train 0.4032836491387246 valid 0.3064098146537112\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 23:\n",
      "LOSS train 0.3859037376259908 valid 0.2975500358475579\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 24:\n",
      "LOSS train 0.3825444713865463 valid 0.3090786871810754\n",
      "Lr is currently [0.01]\n",
      "EPOCH 25:\n",
      "LOSS train 0.37597788628066303 valid 0.30710364567736786\n",
      "Lr is currently [0.01]\n",
      "EPOCH 26:\n",
      "LOSS train 0.371609467307815 valid 0.31201044550269014\n",
      "Lr is currently [0.01]\n",
      "EPOCH 27:\n",
      "LOSS train 0.36301271239075733 valid 0.3233389756383581\n",
      "Lr is currently [0.01]\n",
      "EPOCH 28:\n",
      "LOSS train 0.35828046817622894 valid 0.3075111739906586\n",
      "Lr is currently [0.01]\n",
      "EPOCH 29:\n",
      "LOSS train 0.34767591930046376 valid 0.30212728182474774\n",
      "Lr is currently [0.01]\n",
      "EPOCH 30:\n",
      "LOSS train 0.34659973994926246 valid 0.30042547129818964\n",
      "Lr is currently [0.01]\n",
      "EPOCH 31:\n",
      "LOSS train 0.3345914709574661 valid 0.29298969592329943\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 32:\n",
      "LOSS train 0.33364041435028235 valid 0.28177070335692\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 33:\n",
      "LOSS train 0.3256267143749447 valid 0.31869489369758713\n",
      "Lr is currently [0.01]\n",
      "EPOCH 34:\n",
      "LOSS train 0.3194069728122522 valid 0.28846801530259353\n",
      "Lr is currently [0.01]\n",
      "EPOCH 35:\n",
      "LOSS train 0.31229269884543326 valid 0.28086757302905124\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 36:\n",
      "LOSS train 0.30980566696560463 valid 0.28759834863012657\n",
      "Lr is currently [0.01]\n",
      "EPOCH 37:\n",
      "LOSS train 0.3063819357474025 valid 0.2918447545574357\n",
      "Lr is currently [0.01]\n",
      "EPOCH 38:\n",
      "LOSS train 0.29981325828949434 valid 0.2868259722987811\n",
      "Lr is currently [0.01]\n",
      "EPOCH 39:\n",
      "LOSS train 0.294053499941571 valid 0.27237300673085785\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 40:\n",
      "LOSS train 0.289115875328835 valid 0.2757916046927373\n",
      "Lr is currently [0.01]\n",
      "EPOCH 41:\n",
      "LOSS train 0.27778938912137313 valid 0.28208207558943993\n",
      "Lr is currently [0.01]\n",
      "EPOCH 42:\n",
      "LOSS train 0.2809693957855282 valid 0.2769270065214692\n",
      "Lr is currently [0.01]\n",
      "EPOCH 43:\n",
      "LOSS train 0.2765527752566178 valid 0.27961028972640634\n",
      "Lr is currently [0.01]\n",
      "EPOCH 44:\n",
      "LOSS train 0.2707070503342391 valid 0.2845839935665329\n",
      "Lr is currently [0.01]\n",
      "EPOCH 45:\n",
      "LOSS train 0.26985354900758357 valid 0.2867300362170984\n",
      "Lr is currently [0.01]\n",
      "EPOCH 46:\n",
      "LOSS train 0.2650446306607107 valid 0.274782079944594\n",
      "Lr is currently [0.01]\n",
      "EPOCH 47:\n",
      "LOSS train 0.2579233421216032 valid 0.27797775423257715\n",
      "Lr is currently [0.01]\n",
      "EPOCH 48:\n",
      "LOSS train 0.2581410717492911 valid 0.2830834418742193\n",
      "Lr is currently [0.01]\n",
      "EPOCH 49:\n",
      "LOSS train 0.2494678728796857 valid 0.2743737541604787\n",
      "Lr is currently [0.01]\n",
      "EPOCH 50:\n",
      "LOSS train 0.2483241055451151 valid 0.2870297650935956\n",
      "Lr is currently [0.01]\n",
      "EPOCH 51:\n",
      "LOSS train 0.24590989440380864 valid 0.2801324973762449\n",
      "Lr is currently [0.01]\n",
      "EPOCH 52:\n",
      "LOSS train 0.2402743972788144 valid 0.2743594574017657\n",
      "Lr is currently [0.01]\n",
      "EPOCH 53:\n",
      "LOSS train 0.23690119656196948 valid 0.2829020626636015\n",
      "Lr is currently [0.01]\n",
      "EPOCH 54:\n",
      "LOSS train 0.23823343495317717 valid 0.27498246770766044\n",
      "Lr is currently [0.01]\n",
      "EPOCH 55:\n",
      "LOSS train 0.2267338467102274 valid 0.26710753495313433\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 56:\n",
      "LOSS train 0.2321130417801755 valid 0.28838427654571\n",
      "Lr is currently [0.01]\n",
      "EPOCH 57:\n",
      "LOSS train 0.22228451366878565 valid 0.2952130275920758\n",
      "Lr is currently [0.01]\n",
      "EPOCH 58:\n",
      "LOSS train 0.23142590554626058 valid 0.2713265623897314\n",
      "Lr is currently [0.01]\n",
      "EPOCH 59:\n",
      "LOSS train 0.22054127300104212 valid 0.2732601391358508\n",
      "Lr is currently [0.01]\n",
      "EPOCH 60:\n",
      "LOSS train 0.21650777548086936 valid 0.26109830852753174\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 61:\n",
      "LOSS train 0.21149571000998696 valid 0.2760067925943683\n",
      "Lr is currently [0.01]\n",
      "EPOCH 62:\n",
      "LOSS train 0.20633455730128394 valid 0.2831036952168991\n",
      "Lr is currently [0.01]\n",
      "EPOCH 63:\n",
      "LOSS train 0.20919574451174397 valid 0.28900988907682607\n",
      "Lr is currently [0.01]\n",
      "EPOCH 64:\n",
      "LOSS train 0.21019782566513145 valid 0.2727344554409178\n",
      "Lr is currently [0.01]\n",
      "EPOCH 65:\n",
      "LOSS train 0.19918298025206893 valid 0.26107903600980836\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 66:\n",
      "LOSS train 0.2023130137813516 valid 0.26461040925803697\n",
      "Lr is currently [0.01]\n",
      "EPOCH 67:\n",
      "LOSS train 0.1990009063568572 valid 0.2547000337443832\n",
      "New best validation loss\n",
      "Lr is currently [0.01]\n",
      "EPOCH 68:\n",
      "LOSS train 0.19170169226974582 valid 0.2668909219371724\n",
      "Lr is currently [0.01]\n",
      "EPOCH 69:\n",
      "LOSS train 0.19222983289800402 valid 0.2627904140131755\n",
      "Lr is currently [0.01]\n",
      "EPOCH 70:\n",
      "LOSS train 0.1889316811327945 valid 0.2583460836322047\n",
      "Lr is currently [0.01]\n",
      "EPOCH 71:\n",
      "LOSS train 0.1882731914686201 valid 0.26255419473939884\n",
      "Lr is currently [0.01]\n",
      "EPOCH 72:\n",
      "LOSS train 0.18460225270758227 valid 0.26888010964547804\n",
      "Lr is currently [0.01]\n",
      "EPOCH 73:\n",
      "LOSS train 0.1832368321683598 valid 0.27522256859811023\n",
      "Lr is currently [0.01]\n",
      "EPOCH 74:\n",
      "LOSS train 0.18055616212150036 valid 0.2701083875920934\n",
      "Lr is currently [0.01]\n",
      "EPOCH 75:\n",
      "LOSS train 0.17689750234042617 valid 0.2754921259151565\n",
      "Lr is currently [0.01]\n",
      "EPOCH 76:\n",
      "LOSS train 0.17627223800112782 valid 0.2754100942854873\n",
      "Lr is currently [0.01]\n",
      "EPOCH 77:\n",
      "LOSS train 0.17467344475947402 valid 0.26619724125420263\n",
      "Lr is currently [0.01]\n",
      "EPOCH 78:\n",
      "LOSS train 0.17382104236856602 valid 0.27701903233860825\n",
      "Lr is currently [0.01]\n",
      "EPOCH 79:\n",
      "LOSS train 0.1690002889735396 valid 0.27048501206607195\n",
      "Lr is currently [0.01]\n",
      "EPOCH 80:\n",
      "LOSS train 0.17117008080245524 valid 0.2743778985443189\n",
      "Lr is currently [0.01]\n",
      "EPOCH 81:\n",
      "LOSS train 0.16967355240061713 valid 0.2654111682657256\n",
      "Lr is currently [0.01]\n",
      "EPOCH 82:\n",
      "LOSS train 0.16230012665612661 valid 0.27592075837957153\n",
      "Lr is currently [0.01]\n",
      "EPOCH 83:\n",
      "LOSS train 0.16686713395179778 valid 0.27572143050040016\n",
      "Lr is currently [0.01]\n",
      "EPOCH 84:\n",
      "LOSS train 0.15179321959714315 valid 0.25156647766966167\n",
      "New best validation loss\n",
      "Lr is currently [0.005]\n",
      "EPOCH 85:\n",
      "LOSS train 0.142139809330581 valid 0.2654199095882682\n",
      "Lr is currently [0.005]\n",
      "EPOCH 86:\n",
      "LOSS train 0.14129188352727015 valid 0.26277456459744525\n",
      "Lr is currently [0.005]\n",
      "EPOCH 87:\n",
      "LOSS train 0.14661953175950687 valid 0.2605322687530942\n",
      "Lr is currently [0.005]\n",
      "EPOCH 88:\n",
      "LOSS train 0.13959433945655025 valid 0.26239333636153284\n",
      "Lr is currently [0.005]\n",
      "EPOCH 89:\n",
      "LOSS train 0.1460611331618841 valid 0.258499627693608\n",
      "Lr is currently [0.005]\n",
      "EPOCH 90:\n",
      "LOSS train 0.14311444722033026 valid 0.2618865181543192\n",
      "Lr is currently [0.005]\n",
      "EPOCH 91:\n",
      "LOSS train 0.14030459429446468 valid 0.2597640395914722\n",
      "Lr is currently [0.005]\n",
      "EPOCH 92:\n",
      "LOSS train 0.13640723421439563 valid 0.2536035707485603\n",
      "Lr is currently [0.005]\n",
      "EPOCH 93:\n",
      "LOSS train 0.14166445740579497 valid 0.25739027101210216\n",
      "Lr is currently [0.005]\n",
      "EPOCH 94:\n",
      "LOSS train 0.13791451721404469 valid 0.27643952547158634\n",
      "Lr is currently [0.005]\n",
      "EPOCH 95:\n",
      "LOSS train 0.1374340492626341 valid 0.2585093481466174\n",
      "Lr is currently [0.005]\n",
      "EPOCH 96:\n",
      "LOSS train 0.13708827190333459 valid 0.26438340104262653\n",
      "Lr is currently [0.005]\n",
      "EPOCH 97:\n",
      "LOSS train 0.13437451678587492 valid 0.26416782491206603\n",
      "Lr is currently [0.005]\n",
      "EPOCH 98:\n",
      "LOSS train 0.1327338452950817 valid 0.25210929087026873\n",
      "Lr is currently [0.005]\n",
      "EPOCH 99:\n",
      "LOSS train 0.1348673032476403 valid 0.26378191662176204\n",
      "Lr is currently [0.005]\n",
      "EPOCH 100:\n",
      "LOSS train 0.13629313839603638 valid 0.2601037102156422\n",
      "Lr is currently [0.005]\n",
      "Average time taken per epoch is 106.9315993580001s\n"
     ]
    }
   ],
   "source": [
    "train(100,model,weeds,50,'convnext',0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e77adee-efc7-4d8c-8c40-68ecd19c2110",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6364bec0-66bb-4319-be0d-c84cfa95b51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.join(wd,'convnext','model_84')\n",
    "model.load_state_dict(torch.load(model_path, weights_only = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d546733-f8a5-4c65-9504-353ae40a7fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 83   1   0   1   1   1   1   5  13]\n",
      " [  4  88   0   0   0   0   1   1   6]\n",
      " [  0   0  95   0   2   0   0   0   1]\n",
      " [  1   1   3  78   0   0   1   2  11]\n",
      " [  1   0   1   2 108   0   0   0   4]\n",
      " [  1   0   0   0   0  93   0   0   8]\n",
      " [  0   1   0   0   0   0  85   1   7]\n",
      " [  9   2   0   0   1   1   0  76   3]\n",
      " [  6   4   2   2   7   5   9   8 903]]\n",
      "Precision for class Chinee apple: 0.7905\n",
      "Recall for class Chinee apple: 0.7830\n",
      "F1-score for class Chinee apple: 0.7867\n",
      "Precision for class Lantana: 0.9072\n",
      "Recall for class Lantana: 0.8800\n",
      "F1-score for class Lantana: 0.8934\n",
      "Precision for class Parkinsonia: 0.9406\n",
      "Recall for class Parkinsonia: 0.9694\n",
      "F1-score for class Parkinsonia: 0.9548\n",
      "Precision for class Parthenium: 0.9398\n",
      "Recall for class Parthenium: 0.8041\n",
      "F1-score for class Parthenium: 0.8667\n",
      "Precision for class Prickly acacia: 0.9076\n",
      "Recall for class Prickly acacia: 0.9310\n",
      "F1-score for class Prickly acacia: 0.9191\n",
      "Precision for class Rubber vine: 0.9300\n",
      "Recall for class Rubber vine: 0.9118\n",
      "F1-score for class Rubber vine: 0.9208\n",
      "Precision for class Siam weed: 0.8763\n",
      "Recall for class Siam weed: 0.9043\n",
      "F1-score for class Siam weed: 0.8901\n",
      "Precision for class Snake weed: 0.8172\n",
      "Recall for class Snake weed: 0.8261\n",
      "F1-score for class Snake weed: 0.8216\n",
      "Precision for class Negative: 0.9446\n",
      "Recall for class Negative: 0.9545\n",
      "F1-score for class Negative: 0.9495\n",
      "Total Accuracy is 91.89034837235866%\n",
      "Time taken is 7.301837600010913\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    evaluate(model, weeds, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fee680-05cb-44cf-91fc-c53e8cb477fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
