{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacfff18-b56d-47cc-8021-86b8f8941c67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cfe941-a575-4497-82e3-196b6e92b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import functional\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "009fad0a-1e6e-492d-894b-d7c60e027a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75a30476-d84d-4454-b282-1c16ad69849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790544f-fb7d-45d9-86b4-8cb420f61398",
   "metadata": {},
   "source": [
    "## Import Swin_V2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b99fd7-d9af-4281-af60-0f49ed07b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9b2801-09fd-44b7-90ae-aa2b11d48167",
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_model = swin_v2_t(weights=Swin_V2_T_Weights.DEFAULT)\n",
    "preprocess = Swin_V2_T_Weights.DEFAULT.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958d5aad-f846-4847-86ad-9a1519cab348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[256]\n",
       "    resize_size=[260]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0dc49c-a3a2-45e5-be4a-492c7f10d45b",
   "metadata": {},
   "source": [
    "## Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67bfc83f-fb88-43c1-911c-74905af090bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class representing a set\n",
    "class WeedsDataset():\n",
    "    def __init__(self, labels, img_dir, transform = None):\n",
    "        self.labels = pd.read_csv(labels)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.classes = self.labels[['Label','Species']].drop_duplicates().sort_values(by = 'Label').reset_index(drop = True)['Species']\n",
    "        self.data = []\n",
    "        self.train = []\n",
    "        self.validation = []\n",
    "        self.test = []\n",
    "        \n",
    "    # Load entire dataset into memory\n",
    "    def __loaddata__(self):\n",
    "        for row in self.labels.itertuples():\n",
    "            filename = row.Filename\n",
    "            label = torch.tensor(row.Label)\n",
    "            img_path = os.path.join(self.img_dir, filename)\n",
    "            image = self.transform(torchvision.io.read_image(img_path))\n",
    "            self.data.append([image,label])\n",
    "        del self.labels\n",
    "        print('Data has been loaded')\n",
    "\n",
    "    # Split into train/validation/test\n",
    "    def __split__(self):\n",
    "        random.shuffle(self.data)\n",
    "        n = len(self.data)\n",
    "        n_train = round(n * 0.8)\n",
    "        n_valid = round(n * 0.1)\n",
    "        \n",
    "        self.train = self.data[:n_train]\n",
    "        self.valid = self.data[n_train:n_train + n_valid]\n",
    "        self.test = self.data[n_train + n_valid:]\n",
    "        del self.data\n",
    "        \n",
    "        print('Data has been split')\n",
    "        print('Size of training set is {}'.format(len(self.train)))\n",
    "        print('Size of validation set is {}'.format(len(self.valid)))\n",
    "        print('Size of test set is {}'.format(len(self.test)))\n",
    "\n",
    "    # Augment with rotations and gaussian noise\n",
    "    def __augment__(self, frac):\n",
    "        n = round(len(self.train) * frac / 5)\n",
    "        rotate_90 = random.sample(self.train, n)\n",
    "        for row in rotate_90:\n",
    "            image = row[0]\n",
    "            row[0] = functional.rotate(image, 90)\n",
    "        rotate_180 = random.sample(self.train, n)\n",
    "        for row in rotate_180:\n",
    "            image = row[0]\n",
    "            row[0] = functional.rotate(image, 180)\n",
    "        rotate_270 = random.sample(self.train,n)\n",
    "        for row in rotate_270:\n",
    "            image = row[0]\n",
    "            row[0] = functional.rotate(image, 270)\n",
    "        self.train += rotate_90 + rotate_180 + rotate_270\n",
    "        gaussian = v2.GaussianNoise(0,0.08,False)\n",
    "        noise = random.sample(self.train, 2*n)\n",
    "        for row in noise:\n",
    "            image = row[0]\n",
    "            row[0] = gaussian(image)\n",
    "        self.train += noise\n",
    "        \n",
    "        print('Data has been augmented')\n",
    "        print('Size of training set is {}'.format(len(self.train)))\n",
    "        \n",
    "    # Get item from sets, format is (image,label)\n",
    "    def __getitem__(self, idx, split):\n",
    "        if split == 'train':\n",
    "            item = self.train[idx]\n",
    "        elif split == 'valid':\n",
    "            item = self.valid[idx]\n",
    "        elif split == 'test':\n",
    "            item = self.test[idx]\n",
    "        return item[0], item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd999b99-02f7-4403-be23-349c41b8749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded\n",
      "Data has been split\n",
      "Size of training set is 72\n",
      "Size of validation set is 9\n",
      "Size of test set is 9\n",
      "Data has been augmented\n",
      "Size of training set is 107\n"
     ]
    }
   ],
   "source": [
    "# Run functions to load and process data\n",
    "weeds = WeedsDataset(rf'{wd}\\mini_labels.csv', rf'{wd}\\images', preprocess)\n",
    "weeds.__loaddata__()\n",
    "weeds.__split__()\n",
    "weeds.__augment__(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f2277d8-dcef-4aa8-974f-c722af4be2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to convert the label back into the class\n",
    "def convert_label(label, dataset):\n",
    "    return dataset.classes[label.item()]\n",
    "\n",
    "def convert_labels(labels,dataset):\n",
    "    lst = []\n",
    "    for label in labels:\n",
    "        data_class = convert_label(label,dataset)\n",
    "        lst.append(data_class)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc68ac7-2023-4c93-9d91-f3b3e2f28c06",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "449cc24d-c853-480c-bf5d-6897bc849725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sends image and label tensors to gpu\n",
    "def get_batch(dataset, current_index, batch_size):\n",
    "    images,labels = zip(*dataset[current_index:current_index+batch_size])\n",
    "    return torch.stack(images, dim = 0), torch.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34bdffe8-2ea1-4c78-b6ff-0efcbc2d7570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that everything is working properly\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "#images,labels = get_batch(weeds.train, 1000, 32)\n",
    "#img_grid = torchvision.utils.make_grid(images.cpu())\n",
    "#matplotlib_imshow(img_grid, one_channel=True)\n",
    "#print(convert_labels(labels.cpu(),weeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7181340a-fd33-4dd8-bb35-9fcb14eb2d9d",
   "metadata": {},
   "source": [
    "## Replacing Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e8c9987-13f4-46b6-be18-8c514a06434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in swin_model.parameters():\n",
    "    param.requires_grad = False\n",
    "layers = []\n",
    "layers.append(nn.Linear(in_features = 768, out_features = 256, bias = True))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(in_features = 256, out_features = 9, bias = True))\n",
    "swin_model.head = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be676306-7099-484a-9c63-7aea30ffd332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#info.summary(swin_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8107d-61d6-4adf-9707-3a1b15906aa8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8c2928d-9ce1-42ef-9e38-3cf881cfa3a5",
   "metadata": {},
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(swin_model.parameters(), lr=0.0012, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fb6697b-35ff-4033-8711-e3e967615682",
   "metadata": {},
   "source": [
    "def train_one_epoch(batch_size):\n",
    "    batches = math.floor(len(weeds.train)/batch_size)\n",
    "    current_index = 0\n",
    "    running_loss = 0\n",
    "    random.shuffle(weeds.train)\n",
    "\n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(weeds.train, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = swin_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.cpu().item()\n",
    "\n",
    "    remainder = len(weeds.train)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(weeds.train, current_index, batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = swin_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.cpu().item()\n",
    "        batches += 1\n",
    "    \n",
    "    return running_loss/batches"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a18d627-a1e5-4ce7-9830-89f5151ca212",
   "metadata": {},
   "source": [
    "def validate(batch_size):\n",
    "    batches = math.floor(len(weeds.valid)/batch_size)\n",
    "    current_index = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(weeds.valid, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        output = swin_model(inputs)\n",
    "        running_loss += loss_fn(output, labels).cpu().item()\n",
    "\n",
    "    remainder = len(weeds.valid)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(weeds.valid, current_index, batch_size)\n",
    "        output = swin_model(inputs)\n",
    "        running_loss += loss_fn(output, labels).cpu().item()\n",
    "        batches += 1\n",
    "    \n",
    "    return running_loss/batches"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69ee8c43-a110-4320-964b-59896c74820e",
   "metadata": {},
   "source": [
    "def train(epochs, batch_size, epoch_counter, folder_name, save):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    current_vloss = 0\n",
    "    start = time.perf_counter()\n",
    "    rounds = 1\n",
    "    \n",
    "    if not os.path.exists('{}\\{}'.format(wd,folder_name)):\n",
    "        os.makedirs('{}\\{}'.format(wd,folder_name))\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}:'.format(epoch_counter + 1))\n",
    "        swin_model.train()\n",
    "        avg_loss = train_one_epoch(batch_size)\n",
    "        \n",
    "        swin_model.eval()\n",
    "        with torch.no_grad():\n",
    "            current_vloss = validate(batch_size)\n",
    "\n",
    "        # Only save last {save} rounds\n",
    "        if rounds + save >= epochs:\n",
    "            model_path = '{}\\model_{}_{}'.format(folder_name, timestamp, epoch_counter + 1)\n",
    "            torch.save(swin_model.state_dict(), model_path)\n",
    "        rounds += 1\n",
    "\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, current_vloss))\n",
    "        epoch_counter += 1\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print('Average time taken per epoch is {}s'.format((end - start)/epochs))\n",
    "    return"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba513019-9de0-41ef-a528-a16486a1c623",
   "metadata": {},
   "source": [
    "train(75,50,0,'finalmodel',10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9e52939-b57d-494b-8965-9ad05792021d",
   "metadata": {},
   "source": [
    "# Run another 10 epochs to see results\n",
    "train(10,50,75,'finalmodel',10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1eed1e11-620d-4927-87fd-910b90908b4c",
   "metadata": {},
   "source": [
    "# Epoch 75 has lowest validation loss so continue from there with lower lr and momentum\n",
    "swin_model.load_state_dict(torch.load('finalmodel\\model_20250222_192234_75', weights_only = True))\n",
    "optimizer = torch.optim.SGD(swin_model.parameters(), lr=0.0008, momentum = 0.4)\n",
    "train(10,50,75,'finalmodel',10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88c0a270-ca12-4eb9-a1ac-38e41ea8a376",
   "metadata": {},
   "source": [
    "# Run another 10 epochs with same settings\n",
    "train(10,50,85,'finalmodel',10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0be9a038-a30a-4792-aced-3b656e32a4de",
   "metadata": {},
   "source": [
    "# Continue from epoch 91\n",
    "swin_model.load_state_dict(torch.load('finalmodel\\model_20250222_211532_91', weights_only = True))\n",
    "optimizer = torch.optim.SGD(swin_model.parameters(), lr=0.0004, momentum = 0.1)\n",
    "train(9,50,91,'finalmodel',9)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97dfb59a-7f6f-4d7a-a957-c3401a56a1d5",
   "metadata": {},
   "source": [
    "#Try again with lower lr and momentum\n",
    "swin_model.load_state_dict(torch.load('finalmodel\\model_20250222_211532_91', weights_only = True))\n",
    "optimizer = torch.optim.SGD(swin_model.parameters(), lr=0.0003, momentum = 0)\n",
    "train(9,50,91,'finalmodel',9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e77adee-efc7-4d8c-8c40-68ecd19c2110",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc5be5cb-5ba5-4a37-84ba-10c2de147534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowest validation loss is 0.2069510916610145 at epoch 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c14e663-8902-476c-891a-70bddad2430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\simzi\\AppData\\Local\\Temp\\ipykernel_41556\\1757134040.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  swin_model.load_state_dict(torch.load('finalmodel\\model_20250222_211532_91', weights_only = True, map_location = torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_model.load_state_dict(torch.load('finalmodel\\model_20250222_211532_91', weights_only = True, map_location = torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2619715-3bf2-450a-8c14-4e7faa2ac5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(batch_size):\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    batches = math.floor(len(weeds.test)/batch_size)\n",
    "    current_index = 0\n",
    "    \n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(weeds.train, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        prediction = torch.argmax(swin_model(inputs), dim = 1)\n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += batch_size\n",
    "\n",
    "    remainder = len(weeds.test)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(weeds.train, current_index, remainder)\n",
    "        prediction = torch.argmax(swin_model(inputs), dim = 1)\n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += remainder\n",
    "\n",
    "    accuracy = 100*correct/total\n",
    "    print ('Accuracy is {}%'.format(accuracy))\n",
    "    end = time.perf_counter()\n",
    "    print('Time taken is {}'.format(end-start))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "191d8094-cd71-4366-b506-ab81b73c70df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 88.88888888888889%\n",
      "Time taken is 0.9284983000252396\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    swin_model.eval()\n",
    "    evaluate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b24e22cf-5713-4d87-9445-b24f4a203015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weeds.train[0][1] # actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a53d0a7-fc36-47fc-8b4e-952103cbbdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(swin_model(get_batch(weeds.train,0,1)[0])) # prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19dd7973-1d8d-40b2-a5f5-a581786f5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy is 92.58%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49e72d4c-a750-4ca6-81ce-8784c082b6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\simzi\\AppData\\Local\\Temp\\ipykernel_41556\\3095721756.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  torch.save(swin_model, 'trained_model\\weedsv1.pt')\n",
      "C:\\Users\\simzi\\AppData\\Local\\Temp\\ipykernel_41556\\3095721756.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  torch.save(swin_model, 'trained_model\\weedsv1.pt')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory trained_model does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mswin_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrained_model\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mweedsv1.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:943\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    940\u001b[39m _check_save_filelike(f)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    944\u001b[39m         _save(\n\u001b[32m    945\u001b[39m             obj,\n\u001b[32m    946\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    949\u001b[39m             _disable_byteorder_record,\n\u001b[32m    950\u001b[39m         )\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:810\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    809\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:781\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    777\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    778\u001b[39m         torch._C.PyTorchFileWriter(\u001b[38;5;28mself\u001b[39m.file_stream, _compute_crc32)\n\u001b[32m    779\u001b[39m     )\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_compute_crc32\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Parent directory trained_model does not exist."
     ]
    }
   ],
   "source": [
    "torch.save(swin_model, 'trained_model\\weedsv1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42170a9-ef85-4f97-99b6-6b18e394d4cd",
   "metadata": {},
   "source": [
    "## Evaluation for F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdefcf59-5d76-43a1-9022-02e779c21c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Formula for F1: (2 * FP) / (2 * TP + FP + FN) OR 2 * (Precision * Recall) / (Precision + Recall)\n",
    "# Formula for Precision: TP / (TP + FP)\n",
    "# Formula for Recall: TP / (TP + FN)\n",
    "\n",
    "num_classes = len(weeds.classes)\n",
    "num_classes # Verify if there are 9 classes\n",
    "\n",
    "def evaluate_f1(batch_size):\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    batches = math.floor(len(weeds.test) / batch_size)\n",
    "    current_index = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(batches):\n",
    "        inputs, labels = get_batch(weeds.train, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        prediction = torch.argmax(swin_model(inputs), dim=1)\n",
    "        \n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += batch_size\n",
    "\n",
    "        all_preds.extend(prediction.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "    # Handle remainder batch\n",
    "    remainder = len(weeds.test) % batch_size\n",
    "    if remainder != 0:\n",
    "        inputs, labels = get_batch(weeds.train, current_index, remainder)\n",
    "        prediction = torch.argmax(swin_model(inputs), dim=1)\n",
    "        \n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += remainder\n",
    "\n",
    "        all_preds.extend(prediction.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy is {}%'.format(accuracy))\n",
    "\n",
    "    # Compute Micro-Averaged F1-Score\n",
    "    num_classes = len(weeds.classes) \n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds, labels=np.arange(num_classes))\n",
    "    print(conf_matrix)\n",
    "\n",
    "    total_TP = 0\n",
    "    total_FP = 0\n",
    "    total_FN = 0\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        TP = conf_matrix[i, i]  # True Positives\n",
    "        FP = np.sum(conf_matrix[:, i]) - TP  # False Positives\n",
    "        FN = np.sum(conf_matrix[i, :]) - TP  # False Negatives\n",
    "\n",
    "        total_TP += TP\n",
    "        total_FP += FP\n",
    "        total_FN += FN\n",
    "\n",
    "    micro_precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0\n",
    "    micro_recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0\n",
    "    micro_f1 = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
    "\n",
    "    print(f\"Micro Precision: {micro_precision:.4f}\")\n",
    "    print(f\"Micro Recall: {micro_recall:.4f}\")\n",
    "    print(f\"Micro F1-score: {micro_f1:.4f}\")\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print('Time taken is {}'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8edd28b3-db47-4ed9-a0ef-fce3bba54dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 88.88888888888889%\n",
      "9\n",
      "[[2 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1]\n",
      " [0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1]]\n",
      "Micro Precision: 0.8889\n",
      "Micro Recall: 0.8889\n",
      "Micro F1-score: 0.8889\n",
      "Time taken is 1.0021111000096425\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    swin_model.eval()\n",
    "    evaluate_f1(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc1a702-7735-48e9-8057-b77d0ed5e7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
