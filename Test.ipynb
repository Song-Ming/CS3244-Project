{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacfff18-b56d-47cc-8021-86b8f8941c67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchinfo as info\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as functional\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009fad0a-1e6e-492d-894b-d7c60e027a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a30476-d84d-4454-b282-1c16ad69849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790544f-fb7d-45d9-86b4-8cb420f61398",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b99fd7-d9af-4281-af60-0f49ed07b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a9b2801-09fd-44b7-90ae-aa2b11d48167",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = swin_v2_t(weights=Swin_V2_T_Weights.DEFAULT).to(device)\n",
    "process = Swin_V2_T_Weights.DEFAULT.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "958d5aad-f846-4847-86ad-9a1519cab348",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "layers = []\n",
    "layers.append(nn.Linear(in_features = 768, out_features = 256, bias = True))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(in_features = 256, out_features = 9, bias = True))\n",
    "model.head = nn.Sequential(*layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0dc49c-a3a2-45e5-be4a-492c7f10d45b",
   "metadata": {},
   "source": [
    "## Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67bfc83f-fb88-43c1-911c-74905af090bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeedsDataset():\n",
    "    def __init__(self, labels, img_dir, transform = None):\n",
    "        self.labels = pd.read_csv(labels)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.classes = self.labels[['Label','Species']].drop_duplicates().sort_values(by = 'Label').reset_index(drop = True)['Species']\n",
    "        self.data = []\n",
    "        self.train = []\n",
    "        self.validation = []\n",
    "        self.test = []\n",
    "        \n",
    "    # Load entire dataset into memory\n",
    "    def __loaddata__(self):\n",
    "        for row in self.labels.itertuples():\n",
    "            filename = row.Filename\n",
    "            label = torch.tensor(row.Label)\n",
    "            img_path = os.path.join(self.img_dir, filename)\n",
    "            image = torchvision.io.read_image(img_path)\n",
    "            self.data.append([image,label])\n",
    "        del self.labels\n",
    "        print('Data has been loaded')\n",
    "\n",
    "    # Split into train/validation/test\n",
    "    def __split__(self):\n",
    "        random.shuffle(self.data)\n",
    "        n = len(self.data)\n",
    "        n_train = round(n * 0.8)\n",
    "        n_valid = round(n * 0.1)\n",
    "        \n",
    "        self.train = self.data[:n_train]\n",
    "        self.valid = self.data[n_train:n_train + n_valid]\n",
    "        self.test = self.data[n_train + n_valid:]\n",
    "        del self.data\n",
    "        gc.collect()\n",
    "        \n",
    "        print('Data has been split')\n",
    "        print('Size of training set is {}'.format(len(self.train)))\n",
    "        print('Size of validation set is {}'.format(len(self.valid)))\n",
    "        print('Size of test set is {}'.format(len(self.test)))\n",
    "\n",
    "    # Augment training data\n",
    "    def __augment__(self, frac):\n",
    "        aug = []\n",
    "        color_jitter = transforms.ColorJitter(brightness = 0.3, contrast = 0.2, saturation = 0.3, hue = 0.0)\n",
    "        n = round(len(self.train) * frac / 3)\n",
    "        \n",
    "        rotate_90 = random.sample(self.train, n)\n",
    "        for row in rotate_90:\n",
    "            image = row[0].clone()\n",
    "            image = transforms.functional.rotate(image, 90)\n",
    "            image = color_jitter(image)\n",
    "            row[0] = image\n",
    "        \n",
    "        rotate_180 = random.sample(self.train, n)\n",
    "        for row in rotate_180:\n",
    "            image = row[0].clone()\n",
    "            image = transforms.functional.rotate(image, 180)\n",
    "            image = color_jitter(image)\n",
    "            row[0] = image\n",
    "        \n",
    "        rotate_270 = random.sample(self.train, n)\n",
    "        for row in rotate_270:\n",
    "            image = row[0].clone()\n",
    "            image = transforms.functional.rotate(image, 270)\n",
    "            image = color_jitter(image)\n",
    "            row[0] = image\n",
    "     \n",
    "        self.train += rotate_90 + rotate_180 + rotate_270  \n",
    "        print('Data has been augmented')\n",
    "        print('Size of training set is {}'.format(len(self.train)))\n",
    "\n",
    "    # Apply model transformation\n",
    "    def __apply__(self):\n",
    "        for i in range(len(self.train)):\n",
    "            self.train[i][0] = self.transform(self.train[i][0])\n",
    "        for i in range(len(self.valid)):\n",
    "            self.valid[i][0] = self.transform(self.valid[i][0])\n",
    "        for i in range(len(self.test)):\n",
    "            self.test[i][0] = self.transform(self.test[i][0]) \n",
    "        print('Data has been transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd999b99-02f7-4403-be23-349c41b8749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded\n",
      "Data has been split\n",
      "Size of training set is 14007\n",
      "Size of validation set is 1751\n",
      "Size of test set is 1751\n",
      "Data has been augmented\n",
      "Size of training set is 22410\n",
      "Data has been transformed\n"
     ]
    }
   ],
   "source": [
    "# Run functions to load and process data\n",
    "weeds = WeedsDataset(rf'{wd}\\labels.csv', rf'{wd}\\images', process)\n",
    "weeds.__loaddata__()\n",
    "weeds.__split__()\n",
    "weeds.__augment__(0.6)\n",
    "weeds.__apply__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8107d-61d6-4adf-9707-3a1b15906aa8",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e299c12-151b-457f-92cc-db73288bbc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, current_index, batch_size):\n",
    "    images,labels = zip(*dataset[current_index:current_index+batch_size])\n",
    "    return torch.stack(images, dim = 0).to(device), torch.stack(labels).to(device)\n",
    "    \n",
    "def train_one_epoch(model, dataset, batch_size):\n",
    "    batches = math.floor(len(dataset.train)/batch_size)\n",
    "    current_index = 0\n",
    "    running_loss = 0\n",
    "    random.shuffle(dataset.train)\n",
    "\n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(dataset.train, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.cpu().item()\n",
    "\n",
    "    remainder = len(weeds.train)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(dataset.train, current_index, batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.cpu().item()\n",
    "        batches += 1\n",
    "    \n",
    "    return running_loss/batches\n",
    "\n",
    "def validate(model, dataset, batch_size):\n",
    "    batches = math.floor(len(weeds.valid)/batch_size)\n",
    "    current_index = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(dataset.valid, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        output = model(inputs)\n",
    "        running_loss += loss_fn(output, labels).cpu().item()\n",
    "\n",
    "    remainder = len(weeds.valid)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(dataset.valid, current_index, batch_size)\n",
    "        output = model(inputs)\n",
    "        running_loss += loss_fn(output, labels).cpu().item()\n",
    "        batches += 1\n",
    "    \n",
    "    return running_loss/batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68045c7b-12be-4fa7-ac5a-e5e1e317affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, dataset, batch_size, folder_name):\n",
    "    best_vloss = 1000\n",
    "    current_vloss = 0\n",
    "    epoch_counter = 1\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    if not os.path.exists('{}\\{}'.format(wd,folder_name)):\n",
    "        os.makedirs('{}\\{}'.format(wd,folder_name))\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}:'.format(epoch_counter))\n",
    "        model.train()\n",
    "        avg_loss = train_one_epoch(model, dataset, batch_size)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            current_vloss = validate(model, dataset, batch_size)\n",
    "\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, current_vloss))\n",
    "        if current_vloss <= best_vloss:\n",
    "                best_vloss = current_vloss\n",
    "                print('New best validation loss')\n",
    "        print('Lr is currently {}'.format(scheduler.get_last_lr()))\n",
    "        scheduler.step(current_vloss)\n",
    "\n",
    "        if epoch_counter > epochs/4:\n",
    "            model_path = '{}\\model_{}'.format(folder_name, epoch_counter)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "        epoch_counter += 1\n",
    "\n",
    "    print('Average time taken per epoch is {}s'.format((end - start)/epochs))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8577d8-1d26-4b6a-9c8b-7e16335069f9",
   "metadata": {},
   "source": [
    "## Training with only classifier unfrozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f73b171-8ffe-4d09-ae58-2462d99cf720",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.012, momentum = 0.8)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a8c84aa-7c06-4179-980e-0bb63ee85038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 1.08932538446711 valid 0.6715267557236884\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 2:\n",
      "LOSS train 0.7961166400155405 valid 0.5107650498135222\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 3:\n",
      "LOSS train 0.7126963263365101 valid 0.4574437278012435\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 4:\n",
      "LOSS train 0.6614561003937753 valid 0.4255613883336385\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 5:\n",
      "LOSS train 0.6214973850213075 valid 0.401267326850858\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 6:\n",
      "LOSS train 0.5964046476412987 valid 0.3663164745602343\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 7:\n",
      "LOSS train 0.5758911241136309 valid 0.35705515473253197\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 8:\n",
      "LOSS train 0.5440605483899403 valid 0.32772717216155595\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 9:\n",
      "LOSS train 0.5270892139067894 valid 0.32824807345039314\n",
      "Lr is currently [0.012]\n",
      "EPOCH 10:\n",
      "LOSS train 0.5119236404686038 valid 0.307759049658974\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 11:\n",
      "LOSS train 0.500695875648799 valid 0.29771579393289155\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 12:\n",
      "LOSS train 0.4755885764989662 valid 0.304717142548826\n",
      "Lr is currently [0.012]\n",
      "EPOCH 13:\n",
      "LOSS train 0.4647617443300833 valid 0.3093342969918417\n",
      "Lr is currently [0.012]\n",
      "EPOCH 14:\n",
      "LOSS train 0.4620657423490405 valid 0.2785658811270777\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 15:\n",
      "LOSS train 0.4376064904017544 valid 0.29758712908046114\n",
      "Lr is currently [0.012]\n",
      "EPOCH 16:\n",
      "LOSS train 0.43483052802643424 valid 0.3465175955142412\n",
      "Lr is currently [0.012]\n",
      "EPOCH 17:\n",
      "LOSS train 0.42705278861894375 valid 0.30464791196088\n",
      "Lr is currently [0.012]\n",
      "EPOCH 18:\n",
      "LOSS train 0.4171345210686027 valid 0.2857832529892524\n",
      "Lr is currently [0.012]\n",
      "EPOCH 19:\n",
      "LOSS train 0.4040362171181592 valid 0.29428406625003034\n",
      "Lr is currently [0.012]\n",
      "EPOCH 20:\n",
      "LOSS train 0.39464709788295366 valid 0.27987259336320375\n",
      "Lr is currently [0.012]\n",
      "EPOCH 21:\n",
      "LOSS train 0.39256339057251183 valid 0.27448757774093085\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 22:\n",
      "LOSS train 0.38432136506572334 valid 0.25680919665481067\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 23:\n",
      "LOSS train 0.3787845321381278 valid 0.2729026741322337\n",
      "Lr is currently [0.012]\n",
      "EPOCH 24:\n",
      "LOSS train 0.3742215650151992 valid 0.27892600843268966\n",
      "Lr is currently [0.012]\n",
      "EPOCH 25:\n",
      "LOSS train 0.3738228652153355 valid 0.2856374888991316\n",
      "Lr is currently [0.012]\n",
      "EPOCH 26:\n",
      "LOSS train 0.35708465359126007 valid 0.2528659775077055\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 27:\n",
      "LOSS train 0.34693356522074786 valid 0.2775183747160352\n",
      "Lr is currently [0.012]\n",
      "EPOCH 28:\n",
      "LOSS train 0.3462790498190574 valid 0.28523498108390616\n",
      "Lr is currently [0.012]\n",
      "EPOCH 29:\n",
      "LOSS train 0.3425469613387218 valid 0.2617749169423607\n",
      "Lr is currently [0.012]\n",
      "EPOCH 30:\n",
      "LOSS train 0.3386966697456842 valid 0.2618015492076261\n",
      "Lr is currently [0.012]\n",
      "EPOCH 31:\n",
      "LOSS train 0.3340042354601263 valid 0.2785942166721927\n",
      "Lr is currently [0.012]\n",
      "EPOCH 32:\n",
      "LOSS train 0.327290794996748 valid 0.2582017746996523\n",
      "Lr is currently [0.012]\n",
      "EPOCH 33:\n",
      "LOSS train 0.32205805222904166 valid 0.2785412446020119\n",
      "Lr is currently [0.012]\n",
      "EPOCH 34:\n",
      "LOSS train 0.3200560302073281 valid 0.251300892579214\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 35:\n",
      "LOSS train 0.31835647606039896 valid 0.2590724716687368\n",
      "Lr is currently [0.012]\n",
      "EPOCH 36:\n",
      "LOSS train 0.3130142700864637 valid 0.2542788331484189\n",
      "Lr is currently [0.012]\n",
      "EPOCH 37:\n",
      "LOSS train 0.309542589354223 valid 0.2676253477240809\n",
      "Lr is currently [0.012]\n",
      "EPOCH 38:\n",
      "LOSS train 0.31368189570566596 valid 0.2594671503951152\n",
      "Lr is currently [0.012]\n",
      "EPOCH 39:\n",
      "LOSS train 0.2994381693629486 valid 0.2752257049368281\n",
      "Lr is currently [0.012]\n",
      "EPOCH 40:\n",
      "LOSS train 0.3033284586595002 valid 0.26518485938302344\n",
      "Lr is currently [0.012]\n",
      "EPOCH 41:\n",
      "LOSS train 0.30067741900748823 valid 0.27566296213101144\n",
      "Lr is currently [0.012]\n",
      "EPOCH 42:\n",
      "LOSS train 0.2965424295622681 valid 0.2537177148236272\n",
      "Lr is currently [0.012]\n",
      "EPOCH 43:\n",
      "LOSS train 0.2874240318524811 valid 0.23795306583148582\n",
      "New best validation loss\n",
      "Lr is currently [0.012]\n",
      "EPOCH 44:\n",
      "LOSS train 0.28303276910083597 valid 0.2531795836791086\n",
      "Lr is currently [0.012]\n",
      "EPOCH 45:\n",
      "LOSS train 0.28651653123990467 valid 0.2609963718439556\n",
      "Lr is currently [0.012]\n",
      "EPOCH 46:\n",
      "LOSS train 0.28002481463724094 valid 0.3034608507942822\n",
      "Lr is currently [0.012]\n",
      "EPOCH 47:\n",
      "LOSS train 0.2787941988300374 valid 0.2714477696798794\n",
      "Lr is currently [0.012]\n",
      "EPOCH 48:\n",
      "LOSS train 0.2689468658265797 valid 0.2605606601355248\n",
      "Lr is currently [0.012]\n",
      "EPOCH 49:\n",
      "LOSS train 0.26708693745938067 valid 0.2906882519762601\n",
      "Lr is currently [0.012]\n",
      "EPOCH 50:\n",
      "LOSS train 0.2680709632175802 valid 0.27373698945545283\n",
      "Lr is currently [0.012]\n",
      "EPOCH 51:\n",
      "LOSS train 0.27192669893658505 valid 0.2946239437442273\n",
      "Lr is currently [0.012]\n",
      "EPOCH 52:\n",
      "LOSS train 0.26698560290918055 valid 0.2538622703092794\n",
      "Lr is currently [0.012]\n",
      "EPOCH 53:\n",
      "LOSS train 0.26382857357838374 valid 0.26599499602970256\n",
      "Lr is currently [0.012]\n",
      "EPOCH 54:\n",
      "LOSS train 0.2643965592977066 valid 0.27057217615785906\n",
      "Lr is currently [0.012]\n",
      "EPOCH 55:\n",
      "LOSS train 0.24376115114957028 valid 0.24904495981172659\n",
      "Lr is currently [0.006]\n",
      "EPOCH 56:\n",
      "LOSS train 0.23651916766020661 valid 0.2578260900069533\n",
      "Lr is currently [0.006]\n",
      "EPOCH 57:\n",
      "LOSS train 0.23905022127060158 valid 0.2545016094453685\n",
      "Lr is currently [0.006]\n",
      "EPOCH 58:\n",
      "LOSS train 0.233776898934342 valid 0.2911977098685586\n",
      "Lr is currently [0.006]\n",
      "EPOCH 59:\n",
      "LOSS train 0.23201922008680606 valid 0.2577967634424567\n",
      "Lr is currently [0.006]\n",
      "EPOCH 60:\n",
      "LOSS train 0.23312984152234476 valid 0.24258728342829272\n",
      "Lr is currently [0.006]\n",
      "EPOCH 61:\n",
      "LOSS train 0.2286870207114846 valid 0.24166089330133722\n",
      "Lr is currently [0.006]\n",
      "EPOCH 62:\n",
      "LOSS train 0.2251768864956757 valid 0.25721367683985996\n",
      "Lr is currently [0.006]\n",
      "EPOCH 63:\n",
      "LOSS train 0.2298981444806458 valid 0.26095441940399017\n",
      "Lr is currently [0.006]\n",
      "EPOCH 64:\n",
      "LOSS train 0.22636418014698412 valid 0.250635153366602\n",
      "Lr is currently [0.006]\n",
      "EPOCH 65:\n",
      "LOSS train 0.23154276550513067 valid 0.2537312380866044\n",
      "Lr is currently [0.006]\n",
      "EPOCH 66:\n",
      "LOSS train 0.21600242158634492 valid 0.2454881666473941\n",
      "Lr is currently [0.003]\n",
      "EPOCH 67:\n",
      "LOSS train 0.2154074577834532 valid 0.24663876313974875\n",
      "Lr is currently [0.003]\n",
      "EPOCH 68:\n",
      "LOSS train 0.2146285581578789 valid 0.24623604299914506\n",
      "Lr is currently [0.003]\n",
      "EPOCH 69:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweeds\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mswin_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs, model, dataset, batch_size, folder_name)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch_counter))\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 13\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataset, batch_size)\u001b[0m\n\u001b[0;32m     17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 19\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     21\u001b[0m remainder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weeds\u001b[38;5;241m.\u001b[39mtrain)\u001b[38;5;241m%\u001b[39mbatch_size\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remainder \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(120,model,weeds,50,'swin_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e77adee-efc7-4d8c-8c40-68ecd19c2110",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40808750-1857-4057-956e-46b5eccd5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula for F1: (2 * FP) / (2 * TP + FP + FN) OR 2 * (Precision * Recall) / (Precision + Recall)\n",
    "# Formula for Precision: TP / (TP + FP)\n",
    "# Formula for Recall: TP / (TP + FN)\n",
    "def metrics(TP,FP,FN):\n",
    "    if TP > 0:\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1  = TP / (TP + 0.5 * (FP + FN))\n",
    "    else:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        F1  = 0\n",
    "    return precision, recall, F1\n",
    "    \n",
    "def evaluate(model, dataset, batch_size):\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    batches = math.floor(len(dataset.test) / batch_size)\n",
    "    current_index = 0\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(batches):\n",
    "        inputs, labels = get_batch(dataset.test, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        prediction = torch.argmax(model(inputs), dim=1)\n",
    "        \n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += batch_size\n",
    "\n",
    "        all_preds.extend(prediction.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    remainder = len(dataset.test) % batch_size\n",
    "    if remainder != 0:\n",
    "        inputs, labels = get_batch(dataset.test, current_index, remainder)\n",
    "        prediction = torch.argmax(model(inputs), dim=1)\n",
    "        \n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += remainder\n",
    "\n",
    "        all_preds.extend(prediction.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Compute Metrics for individual classes\n",
    "    num_classes = len(dataset.classes) \n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds, labels=np.arange(num_classes))\n",
    "    print(conf_matrix)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        TP = conf_matrix[i, i]  # True Positives\n",
    "        FP = np.sum(conf_matrix[:, i]) - TP  # False Positives\n",
    "        FN = np.sum(conf_matrix[i, :]) - TP  # False Negatives\n",
    "        precision,recall,F1 = metrics(TP, FP, FN)\n",
    "        print(f\"Precision for class {dataset.classes[i]}: {precision:.4f}\")\n",
    "        print(f\"Recall for class {dataset.classes[i]}: {recall:.4f}\")\n",
    "        print(f\"F1-score for class {dataset.classes[i]}: {F1:.4f}\")\n",
    "    print('Total Accuracy is {}%'.format(accuracy))\n",
    "    \n",
    "    end = time.perf_counter()\n",
    "    print('Time taken is {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57ffc5dc-821b-4b41-a3b9-65c1deceef1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.join(wd,'swin_t','model_68')\n",
    "model.load_state_dict(torch.load(model_path, weights_only = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d546733-f8a5-4c65-9504-353ae40a7fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 85   1   0   0   0   0   2  12   6]\n",
      " [  2  88   1   0   1   0   0   2   6]\n",
      " [  0   0  95   1   2   0   0   0   0]\n",
      " [  2   0   0  85   3   0   1   0   6]\n",
      " [  0   0   1   0 113   0   0   0   2]\n",
      " [  0   0   1   0   1  94   0   0   6]\n",
      " [  0   0   0   0   0   0  87   0   7]\n",
      " [  6   0   0   1   0   1   0  81   3]\n",
      " [  4   4   3   6   9   6   6   5 903]]\n",
      "Precision for class Chinee apple: 0.8586\n",
      "Recall for class Chinee apple: 0.8019\n",
      "F1-score for class Chinee apple: 0.8293\n",
      "Precision for class Lantana: 0.9462\n",
      "Recall for class Lantana: 0.8800\n",
      "F1-score for class Lantana: 0.9119\n",
      "Precision for class Parkinsonia: 0.9406\n",
      "Recall for class Parkinsonia: 0.9694\n",
      "F1-score for class Parkinsonia: 0.9548\n",
      "Precision for class Parthenium: 0.9140\n",
      "Recall for class Parthenium: 0.8763\n",
      "F1-score for class Parthenium: 0.8947\n",
      "Precision for class Prickly acacia: 0.8760\n",
      "Recall for class Prickly acacia: 0.9741\n",
      "F1-score for class Prickly acacia: 0.9224\n",
      "Precision for class Rubber vine: 0.9307\n",
      "Recall for class Rubber vine: 0.9216\n",
      "F1-score for class Rubber vine: 0.9261\n",
      "Precision for class Siam weed: 0.9062\n",
      "Recall for class Siam weed: 0.9255\n",
      "F1-score for class Siam weed: 0.9158\n",
      "Precision for class Snake weed: 0.8100\n",
      "Recall for class Snake weed: 0.8804\n",
      "F1-score for class Snake weed: 0.8438\n",
      "Precision for class Negative: 0.9617\n",
      "Recall for class Negative: 0.9545\n",
      "F1-score for class Negative: 0.9581\n",
      "Total Accuracy is 93.14677327241576%\n",
      "Time taken is 5.439295599993784\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    evaluate(model, weeds, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fee680-05cb-44cf-91fc-c53e8cb477fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
