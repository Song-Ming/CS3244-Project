{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dacfff18-b56d-47cc-8021-86b8f8941c67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cfe941-a575-4497-82e3-196b6e92b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import functional\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009fad0a-1e6e-492d-894b-d7c60e027a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a30476-d84d-4454-b282-1c16ad69849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790544f-fb7d-45d9-86b4-8cb420f61398",
   "metadata": {},
   "source": [
    "## Import Swin_V2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b99fd7-d9af-4281-af60-0f49ed07b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a9b2801-09fd-44b7-90ae-aa2b11d48167",
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_model = swin_v2_t(weights=Swin_V2_T_Weights.DEFAULT)\n",
    "preprocess = Swin_V2_T_Weights.DEFAULT.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958d5aad-f846-4847-86ad-9a1519cab348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[256]\n",
       "    resize_size=[260]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0dc49c-a3a2-45e5-be4a-492c7f10d45b",
   "metadata": {},
   "source": [
    "## Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67bfc83f-fb88-43c1-911c-74905af090bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class representing a set\n",
    "class WeedsDataset():\n",
    "    def __init__(self, labels, img_dir, transform = None):\n",
    "        self.labels = pd.read_csv(labels)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.classes = self.labels[['Label','Species']].drop_duplicates().sort_values(by = 'Label').reset_index(drop = True)['Species']\n",
    "        self.data = []\n",
    "        self.train = []\n",
    "        self.validation = []\n",
    "        self.test = []\n",
    "        \n",
    "    # Load entire dataset into memory\n",
    "    def __loaddata__(self):\n",
    "        for row in self.labels.itertuples():\n",
    "            filename = row.Filename\n",
    "            label = torch.tensor(row.Label)\n",
    "            img_path = os.path.join(self.img_dir, filename)\n",
    "            image = self.transform(torchvision.io.read_image(img_path))\n",
    "            self.data.append([image,label])\n",
    "        del self.labels\n",
    "        print('Data has been loaded')\n",
    "\n",
    "    # Split into train/validation/test\n",
    "    def __split__(self):\n",
    "        random.shuffle(self.data)\n",
    "        n = len(self.data)\n",
    "        n_train = round(n * 0.8)\n",
    "        n_valid = round(n * 0.1)\n",
    "        \n",
    "        self.train = self.data[:n_train]\n",
    "        self.valid = self.data[n_train:n_train + n_valid]\n",
    "        self.test = self.data[n_train + n_valid:]\n",
    "        del self.data\n",
    "        \n",
    "        print('Data has been split')\n",
    "        print('Size of training set is {}'.format(len(self.train)))\n",
    "        print('Size of validation set is {}'.format(len(self.valid)))\n",
    "        print('Size of test set is {}'.format(len(self.test)))\n",
    "\n",
    "    # Augment with rotations and gaussian noise\n",
    "    def __augment__(self, frac):\n",
    "        n = round(len(self.train) * frac / 5)\n",
    "        rotate_90 = random.sample(self.train, n)\n",
    "        for row in rotate_90:\n",
    "            image = row[0]\n",
    "            row[0] = functional.rotate(image, 90)\n",
    "        rotate_180 = random.sample(self.train, n)\n",
    "        for row in rotate_180:\n",
    "            image = row[0]\n",
    "            row[0] = functional.rotate(image, 180)\n",
    "        rotate_270 = random.sample(self.train,n)\n",
    "        for row in rotate_270:\n",
    "            image = row[0]\n",
    "            row[0] = functional.rotate(image, 270)\n",
    "        self.train += rotate_90 + rotate_180 + rotate_270\n",
    "        gaussian = v2.GaussianNoise(0,0.08,False)\n",
    "        noise = random.sample(self.train, 2*n)\n",
    "        for row in noise:\n",
    "            image = row[0]\n",
    "            row[0] = gaussian(image)\n",
    "        self.train += noise\n",
    "        \n",
    "        print('Data has been augmented')\n",
    "        print('Size of training set is {}'.format(len(self.train)))\n",
    "        \n",
    "    # Get item from sets, format is (image,label)\n",
    "    def __getitem__(self, idx, split):\n",
    "        if split == 'train':\n",
    "            item = self.train[idx]\n",
    "        elif split == 'valid':\n",
    "            item = self.valid[idx]\n",
    "        elif split == 'test':\n",
    "            item = self.test[idx]\n",
    "        return item[0], item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd999b99-02f7-4403-be23-349c41b8749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded\n",
      "Data has been split\n",
      "Size of training set is 72\n",
      "Size of validation set is 9\n",
      "Size of test set is 9\n",
      "Data has been augmented\n",
      "Size of training set is 107\n"
     ]
    }
   ],
   "source": [
    "# Run functions to load and process data\n",
    "weeds = WeedsDataset(rf'{wd}\\mini_labels.csv', rf'{wd}\\images', preprocess)\n",
    "weeds.__loaddata__()\n",
    "weeds.__split__()\n",
    "weeds.__augment__(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f2277d8-dcef-4aa8-974f-c722af4be2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to convert the label back into the class\n",
    "def convert_label(label, dataset):\n",
    "    return dataset.classes[label.item()]\n",
    "\n",
    "def convert_labels(labels,dataset):\n",
    "    lst = []\n",
    "    for label in labels:\n",
    "        data_class = convert_label(label,dataset)\n",
    "        lst.append(data_class)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc68ac7-2023-4c93-9d91-f3b3e2f28c06",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449cc24d-c853-480c-bf5d-6897bc849725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sends image and label tensors to gpu\n",
    "def get_batch(dataset, current_index, batch_size):\n",
    "    images,labels = zip(*dataset[current_index:current_index+batch_size])\n",
    "    return torch.stack(images, dim = 0), torch.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34bdffe8-2ea1-4c78-b6ff-0efcbc2d7570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that everything is working properly\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "#images,labels = get_batch(weeds.train, 1000, 32)\n",
    "#img_grid = torchvision.utils.make_grid(images.cpu())\n",
    "#matplotlib_imshow(img_grid, one_channel=True)\n",
    "#print(convert_labels(labels.cpu(),weeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7181340a-fd33-4dd8-bb35-9fcb14eb2d9d",
   "metadata": {},
   "source": [
    "## Replacing Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e8c9987-13f4-46b6-be18-8c514a06434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in swin_model.parameters():\n",
    "    param.requires_grad = False\n",
    "layers = []\n",
    "layers.append(nn.Linear(in_features = 768, out_features = 256, bias = True))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(in_features = 256, out_features = 9, bias = True))\n",
    "swin_model.head = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be676306-7099-484a-9c63-7aea30ffd332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#info.summary(swin_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8107d-61d6-4adf-9707-3a1b15906aa8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8c2928d-9ce1-42ef-9e38-3cf881cfa3a5",
   "metadata": {},
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(swin_model.parameters(), lr=0.0012, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fb6697b-35ff-4033-8711-e3e967615682",
   "metadata": {},
   "source": [
    "def train_one_epoch(batch_size):\n",
    "    batches = math.floor(len(weeds.train)/batch_size)\n",
    "    current_index = 0\n",
    "    running_loss = 0\n",
    "    random.shuffle(weeds.train)\n",
    "\n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(weeds.train, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = swin_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.cpu().item()\n",
    "\n",
    "    remainder = len(weeds.train)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(weeds.train, current_index, batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = swin_model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.cpu().item()\n",
    "        batches += 1\n",
    "    \n",
    "    return running_loss/batches"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a18d627-a1e5-4ce7-9830-89f5151ca212",
   "metadata": {},
   "source": [
    "def validate(batch_size):\n",
    "    batches = math.floor(len(weeds.valid)/batch_size)\n",
    "    current_index = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(weeds.valid, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        output = swin_model(inputs)\n",
    "        running_loss += loss_fn(output, labels).cpu().item()\n",
    "\n",
    "    remainder = len(weeds.valid)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(weeds.valid, current_index, batch_size)\n",
    "        output = swin_model(inputs)\n",
    "        running_loss += loss_fn(output, labels).cpu().item()\n",
    "        batches += 1\n",
    "    \n",
    "    return running_loss/batches"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69ee8c43-a110-4320-964b-59896c74820e",
   "metadata": {},
   "source": [
    "def train(epochs, batch_size, epoch_counter, folder_name, save):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    current_vloss = 0\n",
    "    start = time.perf_counter()\n",
    "    rounds = 1\n",
    "    \n",
    "    if not os.path.exists('{}\\{}'.format(wd,folder_name)):\n",
    "        os.makedirs('{}\\{}'.format(wd,folder_name))\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}:'.format(epoch_counter + 1))\n",
    "        swin_model.train()\n",
    "        avg_loss = train_one_epoch(batch_size)\n",
    "        \n",
    "        swin_model.eval()\n",
    "        with torch.no_grad():\n",
    "            current_vloss = validate(batch_size)\n",
    "\n",
    "        # Only save last {save} rounds\n",
    "        if rounds + save >= epochs:\n",
    "            model_path = '{}\\model_{}_{}'.format(folder_name, timestamp, epoch_counter + 1)\n",
    "            torch.save(swin_model.state_dict(), model_path)\n",
    "        rounds += 1\n",
    "\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, current_vloss))\n",
    "        epoch_counter += 1\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print('Average time taken per epoch is {}s'.format((end - start)/epochs))\n",
    "    return"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba513019-9de0-41ef-a528-a16486a1c623",
   "metadata": {},
   "source": [
    "train(75,50,0,'finalmodel',10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9e52939-b57d-494b-8965-9ad05792021d",
   "metadata": {},
   "source": [
    "# Run another 10 epochs to see results\n",
    "train(10,50,75,'finalmodel',10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1eed1e11-620d-4927-87fd-910b90908b4c",
   "metadata": {},
   "source": [
    "# Epoch 75 has lowest validation loss so continue from there with lower lr and momentum\n",
    "swin_model.load_state_dict(torch.load('finalmodel\\model_20250222_192234_75', weights_only = True))\n",
    "optimizer = torch.optim.SGD(swin_model.parameters(), lr=0.0008, momentum = 0.4)\n",
    "train(10,50,75,'finalmodel',10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88c0a270-ca12-4eb9-a1ac-38e41ea8a376",
   "metadata": {},
   "source": [
    "# Run another 10 epochs with same settings\n",
    "train(10,50,85,'finalmodel',10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0be9a038-a30a-4792-aced-3b656e32a4de",
   "metadata": {},
   "source": [
    "# Continue from epoch 91\n",
    "swin_model.load_state_dict(torch.load('finalmodel\\model_20250222_211532_91', weights_only = True))\n",
    "optimizer = torch.optim.SGD(swin_model.parameters(), lr=0.0004, momentum = 0.1)\n",
    "train(9,50,91,'finalmodel',9)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97dfb59a-7f6f-4d7a-a957-c3401a56a1d5",
   "metadata": {},
   "source": [
    "#Try again with lower lr and momentum\n",
    "swin_model.load_state_dict(torch.load('finalmodel\\model_20250222_211532_91', weights_only = True))\n",
    "optimizer = torch.optim.SGD(swin_model.parameters(), lr=0.0003, momentum = 0)\n",
    "train(9,50,91,'finalmodel',9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e77adee-efc7-4d8c-8c40-68ecd19c2110",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc5be5cb-5ba5-4a37-84ba-10c2de147534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowest validation loss is 0.2069510916610145 at epoch 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c14e663-8902-476c-891a-70bddad2430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\simzi\\AppData\\Local\\Temp\\ipykernel_8784\\1757134040.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  swin_model.load_state_dict(torch.load('finalmodel\\model_20250222_211532_91', weights_only = True, map_location = torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_model.load_state_dict(torch.load('finalmodel\\model_20250222_211532_91', weights_only = True, map_location = torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2619715-3bf2-450a-8c14-4e7faa2ac5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(batch_size):\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    batches = math.floor(len(weeds.test)/batch_size)\n",
    "    current_index = 0\n",
    "    \n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(weeds.train, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        prediction = torch.argmax(swin_model(inputs), dim = 1)\n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += batch_size\n",
    "\n",
    "    remainder = len(weeds.test)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(weeds.train, current_index, remainder)\n",
    "        prediction = torch.argmax(swin_model(inputs), dim = 1)\n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += remainder\n",
    "\n",
    "    accuracy = 100*correct/total\n",
    "    print ('Accuracy is {}%'.format(accuracy))\n",
    "    end = time.perf_counter()\n",
    "    print('Time taken is {}'.format(end-start))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "191d8094-cd71-4366-b506-ab81b73c70df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 88.88888888888889%\n",
      "Time taken is 2.746477500011679\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    swin_model.eval()\n",
    "    evaluate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b24e22cf-5713-4d87-9445-b24f4a203015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weeds.train[0][1] # actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a53d0a7-fc36-47fc-8b4e-952103cbbdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(swin_model(get_batch(weeds.train,0,1)[0])) # prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19dd7973-1d8d-40b2-a5f5-a581786f5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy is 92.58%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49e72d4c-a750-4ca6-81ce-8784c082b6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\simzi\\AppData\\Local\\Temp\\ipykernel_8784\\3095721756.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  torch.save(swin_model, 'trained_model\\weedsv1.pt')\n",
      "C:\\Users\\simzi\\AppData\\Local\\Temp\\ipykernel_8784\\3095721756.py:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  torch.save(swin_model, 'trained_model\\weedsv1.pt')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory trained_model does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mswin_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrained_model\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mweedsv1.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:943\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    940\u001b[39m _check_save_filelike(f)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    944\u001b[39m         _save(\n\u001b[32m    945\u001b[39m             obj,\n\u001b[32m    946\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    949\u001b[39m             _disable_byteorder_record,\n\u001b[32m    950\u001b[39m         )\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:810\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    809\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\serialization.py:781\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    777\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    778\u001b[39m         torch._C.PyTorchFileWriter(\u001b[38;5;28mself\u001b[39m.file_stream, _compute_crc32)\n\u001b[32m    779\u001b[39m     )\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_compute_crc32\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Parent directory trained_model does not exist."
     ]
    }
   ],
   "source": [
    "torch.save(swin_model, 'trained_model\\weedsv1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42170a9-ef85-4f97-99b6-6b18e394d4cd",
   "metadata": {},
   "source": [
    "## Evaluation for F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefcf59-5d76-43a1-9022-02e779c21c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Formula for F1: (2 * FP) / (2 * TP + FP + FN) OR 2 * (Precision * Recall) / (Precision + Recall)\n",
    "# Formula for Precision: TP / (TP + FP)\n",
    "# Formula for Recall: TP / (TP + FN)\n",
    "\n",
    "num_classes = len(weeds.classes)\n",
    "num_classes # Verify if there are 9 classes\n",
    "\n",
    "def evaluate_f1(batch_size):\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    batches = math.floor(len(weeds.test) / batch_size)\n",
    "    current_index = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Split into batches\n",
    "    for i in range(batches):\n",
    "        inputs, labels = get_batch(weeds.train, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        prediction = torch.argmax(swin_model(inputs), dim=1)\n",
    "        \n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += batch_size\n",
    "\n",
    "        all_preds.extend(prediction.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "    # Handle remainder batch\n",
    "    remainder = len(weeds.test) % batch_size\n",
    "    if remainder != 0:\n",
    "        inputs, labels = get_batch(weeds.train, current_index, remainder)\n",
    "        prediction = torch.argmax(swin_model(inputs), dim=1)\n",
    "        \n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += remainder\n",
    "\n",
    "        all_preds.extend(prediction.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy is {}%'.format(accuracy))\n",
    "\n",
    "    # Compute Micro-Averaged F1-Score\n",
    "    num_classes = len(weeds.classes) \n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds, labels=np.arange(num_classes))\n",
    "    print(conf_matrix)\n",
    "\n",
    "    total_TP = 0\n",
    "    total_FP = 0\n",
    "    total_FN = 0\n",
    "\n",
    "    # Generate confuction matriz\n",
    "    for i in range(num_classes):\n",
    "        TP = conf_matrix[i, i]  # True Positives\n",
    "        FP = np.sum(conf_matrix[:, i]) - TP  # False Positives\n",
    "        FN = np.sum(conf_matrix[i, :]) - TP  # False Negatives\n",
    "\n",
    "        total_TP += TP\n",
    "        total_FP += FP\n",
    "        total_FN += FN\n",
    "\n",
    "    micro_precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0\n",
    "    micro_recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0\n",
    "    micro_f1 = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
    "\n",
    "    print(f\"Micro Precision: {micro_precision:.4f}\")\n",
    "    print(f\"Micro Recall: {micro_recall:.4f}\")\n",
    "    print(f\"Micro F1-score: {micro_f1:.4f}\")\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print('Time taken is {}'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd28b3-db47-4ed9-a0ef-fce3bba54dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    swin_model.eval()\n",
    "    evaluate_f1(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c4828-202d-4da4-addd-6dcbb304bedb",
   "metadata": {},
   "source": [
    "## Augmentation with ColorJitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c0dd92-6768-4244-8c38-abe88766c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Define the ColorJitter and Gaussian transformation\n",
    "color_jitter = transforms.ColorJitter(brightness = 0.3, contrast = 0.2, saturation = 0.3)\n",
    "gaussian = v2.GaussianNoise(0, 0.08, False)\n",
    "\n",
    "def visualize_images(original, augmented):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Convert tensor images to numpy arrays for visualization\n",
    "    original_img = original.permute(1, 2, 0).numpy()  # Convert from C x H x W to H x W x C\n",
    "    augmented_img = augmented.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Normalize the images to [0, 1] range if required (since they may be in [0, 255] or [-1, 1] range)\n",
    "    original_img = np.clip(original_img, 0, 1)\n",
    "    augmented_img = np.clip(augmented_img, 0, 1)\n",
    "\n",
    "    # Display the images\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(augmented_img)\n",
    "    axes[1].set_title('Augmented Image')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show() \n",
    "    plt.close(fig) \n",
    "\n",
    "def augment_cj(batch_size):\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    batches = math.floor(len(weeds.test) / batch_size)\n",
    "    current_index = 0\n",
    "\n",
    "    all_preds = []  \n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(batches):\n",
    "        # Get batch of data\n",
    "        inputs, labels = get_batch(weeds.train, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "\n",
    "        # Check the range before normalization\n",
    "        print(f\"Before normalization: min={inputs.min()}, max={inputs.max()}\")\n",
    "\n",
    "        # Normalize the entire batch (to [0, 1] range)\n",
    "        eps = 1e-8  \n",
    "        inputs = (inputs - inputs.min()) / (inputs.max() - inputs.min() + eps)\n",
    "\n",
    "        # Check after normalization\n",
    "        print(f\"After normalization: min={inputs.min()}, max={inputs.max()}\")\n",
    "\n",
    "        # Select the first image from the batch for visualization\n",
    "        original_image = inputs[0]  \n",
    "        \n",
    "        # Apply ColorJitter to augment the images\n",
    "        cj_image = color_jitter(original_image) \n",
    "        augmented_image = gaussian(cj_image)\n",
    "\n",
    "        # Visualize the original vs augmented image\n",
    "        visualize_images(original_image, augmented_image)\n",
    "        \n",
    "        # Check if any pixel values in inputs are negative\n",
    "        if (inputs < 0).any():\n",
    "            print(f\"Warning: Negative pixel values detected in inputs at batch {i}\")\n",
    "            negative_pixel_indices = np.where(inputs.numpy() < 0)\n",
    "            print(f\"Negative pixel indices at batch {i}: {negative_pixel_indices}\")\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        prediction = torch.argmax(swin_model(inputs), dim=1)\n",
    "\n",
    "        # Check for negative values in the predictions \n",
    "        if (prediction < 0).any():\n",
    "            print(f\"Warning: Negative values detected in predictions at batch {i}\")\n",
    "            \n",
    "        # Store the predictions and labels for further analysis\n",
    "        all_preds.extend(prediction.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "    # Check for negative values in all predictions and labels\n",
    "    if any(val < 0 for val in all_preds):\n",
    "        print(\"Warning: Negative values detected in all_preds!\")\n",
    "    if any(val < 0 for val in all_labels):\n",
    "        print(\"Warning: Negative values detected in all_labels!\")\n",
    "    else:\n",
    "        print(\"There are no negative values detected\")\n",
    "        \n",
    "    end = time.perf_counter()\n",
    "    print('Time taken is {}'.format(end - start))\n",
    "\n",
    "    return all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9dde79-8349-4310-bb91-6b242c49028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    swin_model.eval()\n",
    "    augment_cj(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc521c-3aa2-4330-b795-38850190c17b",
   "metadata": {},
   "source": [
    "## Augmentation with Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3496c6-7b39-42d9-bb08-3da04ab31346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Define the RandomCrop and Gaussian transformation\n",
    "random_crop = transforms.RandomCrop(size=(100, 100)) \n",
    "gaussian = v2.GaussianNoise(0, 0.08, False)\n",
    "\n",
    "def augment_crop(batch_size):\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    batches = math.floor(len(weeds.test) / batch_size)\n",
    "    current_index = 0\n",
    "\n",
    "    all_preds = []  \n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(batches):\n",
    "        # Get batch of data\n",
    "        inputs, labels = get_batch(weeds.train, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "\n",
    "        # Check the range before normalization\n",
    "        print(f\"Before normalization: min={inputs.min()}, max={inputs.max()}\")\n",
    "\n",
    "        # Normalize the entire batch (to [0, 1] range)\n",
    "        eps = 1e-8  \n",
    "        inputs = (inputs - inputs.min()) / (inputs.max() - inputs.min() + eps)\n",
    "\n",
    "        # Check after normalization\n",
    "        print(f\"After normalization: min={inputs.min()}, max={inputs.max()}\")\n",
    "\n",
    "        # Select the first image from the batch for visualization\n",
    "        original_image = inputs[0]  \n",
    "\n",
    "        # Apply RandomCrop to augment the image\n",
    "        cropped_image = random_crop(original_image) \n",
    "        augmented_image = gaussian(cropped_image)\n",
    "\n",
    "        # Visualize the original vs augmented image\n",
    "        visualize_images(original_image, augmented_image)\n",
    "        \n",
    "        # Check if any pixel values in inputs are negative\n",
    "        if (inputs < 0).any():\n",
    "            print(f\"Warning: Negative pixel values detected in inputs at batch {i}\")\n",
    "            negative_pixel_indices = np.where(inputs.numpy() < 0)\n",
    "            print(f\"Negative pixel indices at batch {i}: {negative_pixel_indices}\")\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        prediction = torch.argmax(swin_model(inputs), dim=1)\n",
    "\n",
    "        # Check for negative values in the predictions \n",
    "        if (prediction < 0).any():\n",
    "            print(f\"Warning: Negative values detected in predictions at batch {i}\")\n",
    "            \n",
    "        # Store the predictions and labels for further analysis\n",
    "        all_preds.extend(prediction.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "    # Check for negative values in all predictions and labels\n",
    "    if any(val < 0 for val in all_preds):\n",
    "        print(\"Warning: Negative values detected in all_preds!\")\n",
    "    if any(val < 0 for val in all_labels):\n",
    "        print(\"Warning: Negative values detected in all_labels!\")\n",
    "    else:\n",
    "        print(\"There are no negative values detected\")\n",
    "        \n",
    "    end = time.perf_counter()\n",
    "    print('Time taken is {}'.format(end - start))\n",
    "\n",
    "    return all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442eb7e-1f8b-48f4-b165-37a2ea28fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    swin_model.eval()\n",
    "    augment_crop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a452a41c-94af-499c-bc47-335654352f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6112bf29-db5b-487d-98d6-121e3f6bb6e3",
   "metadata": {},
   "source": [
    "## Inference Time Per Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16ba517a-853c-4490-98bc-fcfd298cbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def inference_time_per_image(model, dataset, num_samples):\n",
    "    model.eval()\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    total_time = 0\n",
    "\n",
    "    print(f\"\\n🔍 Measuring inference time for {num_samples} images...\\n\")\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        image, label = dataset[idx]\n",
    "\n",
    "        # Add batch dimension since model expects batches\n",
    "        input_tensor = image.unsqueeze(0)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            _ = model(input_tensor)\n",
    "\n",
    "        end_time = time.perf_counter()\n",
    "        elapsed_time = end_time - start_time\n",
    "        total_time += elapsed_time\n",
    "\n",
    "        print(f\"Image {i+1} | Inference Time: {elapsed_time:.5f} seconds\")\n",
    "\n",
    "    avg_time = total_time / num_samples\n",
    "    print(f\"\\nAverage Inference Time: {avg_time:.5f} seconds per image\")\n",
    "\n",
    "    return avg_time\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a01a3bd-7632-4e46-855c-b7ad38be8402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Measuring inference time for 9 images...\n",
      "\n",
      "Image 1 | Inference Time: 0.43797 seconds\n",
      "Image 2 | Inference Time: 0.31151 seconds\n",
      "Image 3 | Inference Time: 0.30753 seconds\n",
      "Image 4 | Inference Time: 0.30054 seconds\n",
      "Image 5 | Inference Time: 0.28023 seconds\n",
      "Image 6 | Inference Time: 0.30530 seconds\n",
      "Image 7 | Inference Time: 0.54159 seconds\n",
      "Image 8 | Inference Time: 0.29613 seconds\n",
      "Image 9 | Inference Time: 0.28613 seconds\n",
      "\n",
      "Average Inference Time: 0.34077 seconds per image\n"
     ]
    }
   ],
   "source": [
    "# Run timing for inference on a few test images\n",
    "average_time = inference_time_per_image(swin_model, weeds.test, num_samples=len(weeds.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212745a4-0269-433d-8e1a-f2623e108ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7e552-b59d-422f-ac7f-080e874b02d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
