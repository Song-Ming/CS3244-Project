{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9aca3d-cb6f-4801-9f88-a1c3f43a0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchinfo as info\n",
    "import torchvision.transforms.v2 as v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7069ad6-8762-4612-9cd7-82e1a0a3915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed(600)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d754c8-3fde-4553-a8b9-d001b8e546af",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd()\n",
    "labels_dir = os.path.join(wd,'Labels')\n",
    "img_dir = os.path.join(wd,'Sets')\n",
    "record_path = os.path.join(wd,'Records')\n",
    "model_path = os.path.join(wd,'Models')\n",
    "if not os.path.exists(record_path):\n",
    "    os.makedirs(record_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17a8b66-dbd7-4fd5-bd6e-1174c4b0d2b2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5881e8-ed4f-4259-95f7-fd5cc952824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import convnext_small, ConvNeXt_Small_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba08aef-f362-4541-83f2-7a65a8d6623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convnext_small(weights = ConvNeXt_Small_Weights.IMAGENET1K_V1).to(device)\n",
    "transform = ConvNeXt_Small_Weights.IMAGENET1K_V1.transforms()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5adfb4b0-e729-4634-9b8c-6444abb33d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "layers.append(nn.Linear(in_features = 768, out_features = 256, bias = True))\n",
    "layers.append(nn.Dropout(p = 0.15))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(in_features = 256, out_features = 256, bias = True))\n",
    "layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(in_features = 256, out_features = 9, bias = True))\n",
    "model.classifier[2] = nn.Sequential(*layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c79148-45a2-484e-83c8-4167816000ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.features[7][0].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.features[7][1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.features[7][2].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.classifier[0].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddc0d0eb-2fc3-438d-8393-552d8cdc4c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#info.summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de01fe8-f450-4559-9bd3-d760900f1bfa",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15bcb2ee-cb99-46d3-8d46-2987a5579c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeedsDataset():\n",
    "    def __init__(self, labels_dir, img_dir, transform):\n",
    "        self.train_labels = pd.read_csv(os.path.join(labels_dir,'train.csv'))\n",
    "        self.valid_labels = pd.read_csv(os.path.join(labels_dir,'valid.csv'))\n",
    "        self.test_labels = pd.read_csv(os.path.join(labels_dir,'test.csv'))\n",
    "        self.img_dir = img_dir\n",
    "        self.classes = None\n",
    "        self.counts = {'train':{},'valid':{},'test':{}}\n",
    "        self.train = []\n",
    "        self.valid = []\n",
    "        self.test = []\n",
    "        self.augmented = []\n",
    "        self.transform = transform\n",
    "        \n",
    "    # Load dataset\n",
    "    def __loaddata__(self):\n",
    "        self.classes = self.train_labels[['Label','Class']].drop_duplicates().sort_values(by = 'Label').reset_index(drop = True)['Class']\n",
    "        for key in self.classes.keys():\n",
    "            self.counts['train'][key] = 0\n",
    "            self.counts['valid'][key] = 0\n",
    "            self.counts['test'][key] = 0\n",
    "\n",
    "        for row in self.train_labels.itertuples():\n",
    "            filename = row.Filename\n",
    "            label = row.Label\n",
    "            self.counts['train'][row.Label] += 1\n",
    "            img_path = os.path.join(self.img_dir,'train',filename)\n",
    "            image = torchvision.io.read_image(img_path)\n",
    "            self.train.append([image,label])   \n",
    "        for row in self.valid_labels.itertuples():\n",
    "            filename = row.Filename\n",
    "            label = row.Label\n",
    "            self.counts['valid'][row.Label] += 1\n",
    "            img_path = os.path.join(self.img_dir,'valid',filename)\n",
    "            image = torchvision.io.read_image(img_path)\n",
    "            self.valid.append([image,label])   \n",
    "        for row in self.test_labels.itertuples():\n",
    "            filename = row.Filename\n",
    "            label = row.Label\n",
    "            self.counts['test'][row.Label] += 1\n",
    "            img_path = os.path.join(self.img_dir,'test',filename)\n",
    "            image = torchvision.io.read_image(img_path)\n",
    "            self.test.append([image,label])\n",
    "\n",
    "        del self.train_labels,self.valid_labels,self.test_labels\n",
    "        print('Data has been loaded')\n",
    "\n",
    "    def __apply__(self):\n",
    "        for i in range(len(self.train)):\n",
    "            image = transform(self.train[i][0])\n",
    "            label = torch.tensor(self.train[i][1])\n",
    "            self.train[i] = [image,label]\n",
    "\n",
    "        for i in range(len(self.valid)):\n",
    "            image = transform(self.valid[i][0])\n",
    "            label = torch.tensor(self.valid[i][1])\n",
    "            self.valid[i] = [image,label]\n",
    "\n",
    "        for i in range(len(self.test)):\n",
    "            image = transform(self.test[i][0])\n",
    "            label = torch.tensor(self.test[i][1])\n",
    "            self.test[i] = [image,label]\n",
    "        print('Data has been processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3f57817-58f8-4272-a61a-a1fec70f49c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded\n",
      "Data has been processed\n"
     ]
    }
   ],
   "source": [
    "weeds = WeedsDataset(labels_dir,img_dir,transform)\n",
    "weeds.__loaddata__()\n",
    "weeds.__apply__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ed7446-f62b-46ee-8445-45fcccf69ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(weeds.counts)\n",
    "#print(weeds.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a69f8f-5cff-46ae-9a40-de0b5ea68ae4",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de1d5ec-6cf4-42cc-bb79-da40c3812083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, current_index, batch):\n",
    "    images,labels = zip(*dataset[current_index:current_index+batch])\n",
    "    return torch.stack(images, dim = 0).to(device), torch.stack(labels).to(device)\n",
    "    \n",
    "def train_one_epoch(model, dataset, batch_size):\n",
    "    random.shuffle(dataset.train)\n",
    "    batches = math.floor(len(dataset.train)/batch_size)\n",
    "    current_index = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(dataset.train, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.cpu().item()\n",
    "\n",
    "    remainder = len(dataset.train)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(dataset.train, current_index, remainder)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.cpu().item()\n",
    "        batches += 1\n",
    "    \n",
    "    return running_loss/batches\n",
    "\n",
    "def validate(model, dataset, batch_size):\n",
    "    batches = math.floor(len(dataset.valid)/batch_size)\n",
    "    current_index = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    for i in range(batches):\n",
    "        inputs,labels = get_batch(dataset.valid, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        output = model(inputs)\n",
    "        running_loss += loss_fn(output, labels).cpu().item()\n",
    "\n",
    "    remainder = len(dataset.valid)%batch_size\n",
    "    if remainder != 0:\n",
    "        inputs,labels = get_batch(dataset.valid, current_index, remainder)\n",
    "        output = model(inputs)\n",
    "        running_loss += loss_fn(output, labels).cpu().item()\n",
    "        batches += 1\n",
    "    \n",
    "    return running_loss/batches\n",
    "\n",
    "def train(epochs, model, dataset, batch_size, folder_name, epoch_number, save, record):\n",
    "    best_vloss = 1000\n",
    "    current_vloss = 0\n",
    "    epoch_counter = epoch_number + 1\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    if not os.path.exists('{}\\{}'.format(model_path,folder_name)):\n",
    "        os.makedirs('{}\\{}'.format(model_path,folder_name))\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}:'.format(epoch_counter))\n",
    "        model.train()\n",
    "        avg_loss = train_one_epoch(model, dataset, batch_size)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            current_vloss = validate(model, dataset, batch_size)\n",
    "\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, current_vloss))\n",
    "        if current_vloss <= best_vloss:\n",
    "                best_vloss = current_vloss\n",
    "                print('New best validation loss')\n",
    "\n",
    "        if epoch_counter - epoch_number > epochs - save:\n",
    "            path = '{}\\\\{}\\\\model_{}'.format(model_path, folder_name, epoch_counter)\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "        record.append([epoch_counter,avg_loss,current_vloss])\n",
    "        epoch_counter += 1\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print('Average time taken per epoch is {}s'.format((end - start)/epochs))\n",
    "    return\n",
    "\n",
    "def compute_weights(dataset):\n",
    "    n_samples = len(dataset.train)\n",
    "    n_classes = len(dataset.classes)\n",
    "    weights = torch.zeros(n_classes)\n",
    "    dict = dataset.counts['train']\n",
    "    for i in range(n_classes):\n",
    "        weight = n_samples/(n_classes*dict[i])\n",
    "        weights[i] = weight\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f4c59db-fa15-49b9-9fdb-8d99ae183014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula for F1: (2 * FP) / (2 * TP + FP + FN) OR 2 * (Precision * Recall) / (Precision + Recall)\n",
    "# Formula for Precision: TP / (TP + FP)\n",
    "# Formula for Recall: TP / (TP + FN)\n",
    "def metrics(TP,FP,FN):\n",
    "    if TP > 0:\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        F1  = TP / (TP + 0.5 * (FP + FN))\n",
    "    else:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        F1  = 0\n",
    "    return precision, recall, F1\n",
    "\n",
    "# Use weeds.dataset for dataset\n",
    "def evaluate(model, dataset, batch_size, classes):\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    batches = math.floor(len(dataset) / batch_size)\n",
    "    current_index = 0\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for i in range(batches):\n",
    "        inputs, labels = get_batch(dataset, current_index, batch_size)\n",
    "        current_index += batch_size\n",
    "        prediction = torch.argmax(model(inputs),dim = 1)\n",
    "        \n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += batch_size\n",
    "\n",
    "        all_preds.extend(prediction.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    remainder = len(dataset) % batch_size\n",
    "    if remainder != 0:\n",
    "        inputs, labels = get_batch(dataset, current_index, remainder)\n",
    "        prediction = torch.argmax(model(inputs),dim = 1)\n",
    "        \n",
    "        correct += sum(prediction == labels).item()\n",
    "        total += remainder\n",
    "\n",
    "        all_preds.extend(prediction.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Compute Metrics for individual classes\n",
    "    num_classes = len(classes) \n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds, labels=np.arange(num_classes))\n",
    "    print(conf_matrix)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        TP = conf_matrix[i, i]  # True Positives\n",
    "        FP = np.sum(conf_matrix[:, i]) - TP  # False Positives\n",
    "        FN = np.sum(conf_matrix[i, :]) - TP  # False Negatives\n",
    "        precision,recall,F1 = metrics(TP, FP, FN)\n",
    "        print(f\"Precision for class {classes[i]}: {precision:.4f}\")\n",
    "        print(f\"Recall for class {classes[i]}: {recall:.4f}\")\n",
    "        print(f\"F1-score for class {classes[i]}: {F1:.4f}\")\n",
    "    print('Total Accuracy is {}%'.format(accuracy))\n",
    "    \n",
    "    end = time.perf_counter()\n",
    "    print('Time taken is {}'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178bc7f8-0bb6-4382-8c1a-d56ee4ee55a6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4e73bd1-b4a4-43c3-b10c-6a3d6f5ddf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7066, 1.8397, 1.8639, 1.8888, 1.8639, 1.9167, 1.8441, 1.8865, 0.2140])\n"
     ]
    }
   ],
   "source": [
    "weights = compute_weights(weeds)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3260b971-13f4-41e2-b280-4c9269430412",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = [['Epoch','Training loss','Validation loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e67317bd-0e07-4a2a-8528-b265986b6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight=weights).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.6, weight_decay = 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43a4c755-bb70-45b7-bd49-e8e0b32d4ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 1.0524419199124904 valid 0.45496965868998385\n",
      "New best validation loss\n",
      "EPOCH 2:\n",
      "LOSS train 0.5435984532091551 valid 0.3809312606009386\n",
      "New best validation loss\n",
      "EPOCH 3:\n",
      "LOSS train 0.4313780654867916 valid 0.26075522323786204\n",
      "New best validation loss\n",
      "EPOCH 4:\n",
      "LOSS train 0.358218370914874 valid 0.2171264932317249\n",
      "New best validation loss\n",
      "EPOCH 5:\n",
      "LOSS train 0.3163522473143561 valid 0.20812960960349794\n",
      "New best validation loss\n",
      "EPOCH 6:\n",
      "LOSS train 0.27996219119069027 valid 0.21534565735166356\n",
      "EPOCH 7:\n",
      "LOSS train 0.25299089874601594 valid 0.18943805683214784\n",
      "New best validation loss\n",
      "EPOCH 8:\n",
      "LOSS train 0.22248187748519843 valid 0.18089980270587286\n",
      "New best validation loss\n",
      "EPOCH 9:\n",
      "LOSS train 0.2069950420229957 valid 0.19499526479107848\n",
      "EPOCH 10:\n",
      "LOSS train 0.18769055309184932 valid 0.19017583166517443\n",
      "EPOCH 11:\n",
      "LOSS train 0.17532680382377747 valid 0.18795354822177\n",
      "EPOCH 12:\n",
      "LOSS train 0.16242437979118696 valid 0.18177708503553422\n",
      "EPOCH 13:\n",
      "LOSS train 0.1457441298677975 valid 0.19368262206977707\n",
      "EPOCH 14:\n",
      "LOSS train 0.14189064504211765 valid 0.1655218317252347\n",
      "New best validation loss\n",
      "EPOCH 15:\n",
      "LOSS train 0.1252823762009339 valid 0.18436649624826545\n",
      "EPOCH 16:\n",
      "LOSS train 0.12813905610182427 valid 0.17001761674438998\n",
      "EPOCH 17:\n",
      "LOSS train 0.1150225493279176 valid 0.16932665933933805\n",
      "EPOCH 18:\n",
      "LOSS train 0.11229662132216119 valid 0.16631456106206624\n",
      "EPOCH 19:\n",
      "LOSS train 0.10210266131588619 valid 0.1637133792263724\n",
      "New best validation loss\n",
      "EPOCH 20:\n",
      "LOSS train 0.09607557103743805 valid 0.1784484714380134\n",
      "EPOCH 21:\n",
      "LOSS train 0.09250663702647408 valid 0.21586214949108534\n",
      "EPOCH 22:\n",
      "LOSS train 0.08842652227884862 valid 0.17808715820754484\n",
      "EPOCH 23:\n",
      "LOSS train 0.08924192971536467 valid 0.15801048018353975\n",
      "New best validation loss\n",
      "EPOCH 24:\n",
      "LOSS train 0.08233029728209923 valid 0.1884771182706927\n",
      "EPOCH 25:\n",
      "LOSS train 0.08574909781608371 valid 0.15809405621504252\n",
      "EPOCH 26:\n",
      "LOSS train 0.08071132937540466 valid 0.15703232207540738\n",
      "New best validation loss\n",
      "EPOCH 27:\n",
      "LOSS train 0.07842560791598964 valid 0.1601628859259062\n",
      "EPOCH 28:\n",
      "LOSS train 0.08174566689709688 valid 0.17931408822631179\n",
      "EPOCH 29:\n",
      "LOSS train 0.07384573243549931 valid 0.15811255041478298\n",
      "EPOCH 30:\n",
      "LOSS train 0.06741391754598502 valid 0.15993236251948875\n",
      "Average time taken per epoch is 108.45031897666631s\n"
     ]
    }
   ],
   "source": [
    "train(30,model,weeds,30,'convnext1',0,30,record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19ded961-af8d-43a3-a981-c501c0fa9b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(model_path,'convnext1','model_26')\n",
    "model.load_state_dict(torch.load(path, weights_only = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37e20553-b5b3-41b3-b929-0c1939e23e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 27:\n",
      "LOSS train 0.05691906336218326 valid 0.15456423512462653\n",
      "New best validation loss\n",
      "EPOCH 28:\n",
      "LOSS train 0.05522882436826921 valid 0.15543122461038025\n",
      "EPOCH 29:\n",
      "LOSS train 0.05372771206360492 valid 0.16357609847860444\n",
      "EPOCH 30:\n",
      "LOSS train 0.04702608804753184 valid 0.15635047998808135\n",
      "EPOCH 31:\n",
      "LOSS train 0.05036789104194137 valid 0.15545791777056026\n",
      "EPOCH 32:\n",
      "LOSS train 0.046067043616971386 valid 0.1658397067350067\n",
      "EPOCH 33:\n",
      "LOSS train 0.04514748547509127 valid 0.15998228430359276\n",
      "EPOCH 34:\n",
      "LOSS train 0.041482439573267894 valid 0.17492557657188026\n",
      "EPOCH 35:\n",
      "LOSS train 0.04497176153831856 valid 0.17017528879992916\n",
      "EPOCH 36:\n",
      "LOSS train 0.043835928209261354 valid 0.1668653349684141\n",
      "EPOCH 37:\n",
      "LOSS train 0.04143200936047056 valid 0.16112670465569817\n",
      "EPOCH 38:\n",
      "LOSS train 0.0434895328410771 valid 0.16619544539904474\n",
      "EPOCH 39:\n",
      "LOSS train 0.03749844959678529 valid 0.17462186930603465\n",
      "EPOCH 40:\n",
      "LOSS train 0.04429637525888505 valid 0.1648417283895315\n",
      "Average time taken per epoch is 99.81935488571305s\n"
     ]
    }
   ],
   "source": [
    "record = record[:27]\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.004, momentum = 0.6, weight_decay = 0.00001)\n",
    "train(14,model,weeds,30,'convnext1',26,14,record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da0da4d3-c693-44a4-a518-397a5ed8de71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(record_path,'convnext1.csv'), 'w', newline='') as csvAP:\n",
    "    writer = csv.writer(csvAP)\n",
    "    writer.writerows(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c3a63-5626-4bc8-b719-4fe8d7fe6ff6",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30acc7cb-f14b-4672-b251-5dd509b4c870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(model_path,'convnext1','model_27')\n",
    "model.load_state_dict(torch.load(path, weights_only = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0ff1dd8-06ec-492b-8977-4ec91b435744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 87   2   0   0   0   0   1   7   3]\n",
      " [  0  99   0   0   0   0   1   1   2]\n",
      " [  0   0 120   0   0   0   0   0   0]\n",
      " [  1   0   1  89   1   0   0   0   1]\n",
      " [  0   0   0   1 101   0   0   0   0]\n",
      " [  2   0   0   0   0 104   0   0   2]\n",
      " [  0   3   0   0   0   0 104   0   3]\n",
      " [  3   2   0   0   0   0   0 112   2]\n",
      " [  9   9   0   6   5   5   3   4 855]]\n",
      "Precision for class Chinee apple: 0.8529\n",
      "Recall for class Chinee apple: 0.8700\n",
      "F1-score for class Chinee apple: 0.8614\n",
      "Precision for class Lantana: 0.8609\n",
      "Recall for class Lantana: 0.9612\n",
      "F1-score for class Lantana: 0.9083\n",
      "Precision for class Parkinsonia: 0.9917\n",
      "Recall for class Parkinsonia: 1.0000\n",
      "F1-score for class Parkinsonia: 0.9959\n",
      "Precision for class Parthenium: 0.9271\n",
      "Recall for class Parthenium: 0.9570\n",
      "F1-score for class Parthenium: 0.9418\n",
      "Precision for class Prickly acacia: 0.9439\n",
      "Recall for class Prickly acacia: 0.9902\n",
      "F1-score for class Prickly acacia: 0.9665\n",
      "Precision for class Rubber vine: 0.9541\n",
      "Recall for class Rubber vine: 0.9630\n",
      "F1-score for class Rubber vine: 0.9585\n",
      "Precision for class Siam weed: 0.9541\n",
      "Recall for class Siam weed: 0.9455\n",
      "F1-score for class Siam weed: 0.9498\n",
      "Precision for class Snake weed: 0.9032\n",
      "Recall for class Snake weed: 0.9412\n",
      "F1-score for class Snake weed: 0.9218\n",
      "Precision for class Negative: 0.9850\n",
      "Recall for class Negative: 0.9542\n",
      "F1-score for class Negative: 0.9694\n",
      "Total Accuracy is 95.43118218161051%\n",
      "Time taken is 4.835645600018324\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    evaluate(model,weeds.test,30,weeds.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4f4bb-1a7d-47f1-b2a2-3b58a8d0f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    evaluate(model,weeds.valid,30,weeds.classes)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fc68b3a-28cc-4ee1-a834-e1fb3c73d553",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    evaluate(model,weeds.train,30,weeds.classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
